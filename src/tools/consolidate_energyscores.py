"""
This file contains source code to process and analyse the energy scores generated by the attention mechanisms during
the testing of the model.
    @author: Christoph Metzner
    @email: cmetzner@vols.utk.edu
    @created: 12/09/2022
    @last modified: 12/09/2022
"""

# built-in libraries
import os
import sys
from typing import List, Dict, Tuple
import pickle
import h5py
import time

# installed libraries
import numpy as np
from scipy import stats
import pandas as pd

pd.set_option('display.max_rows', None)
import torch
import matplotlib.pyplot as plt

from transformers import AutoTokenizer

CLF_tokenizer = AutoTokenizer.from_pretrained(
    "/Users/cmetzner/Desktop/Study/PhD/research/ORNL/Biostatistics and Multiscale System Modeling/attention_mechanisms/src/models/Clinical-Longformer",
    model_max_length=4096)

# Set up root folder
try:
    root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
except NameError:
    root = os.path.dirname(os.getcwd())
sys.path.append(root)
print('Project root: {}'.format(root))

# Path to storage location of results
path_results = os.path.join('/Volumes', 'MetznerSSD', 'Study_Attention')

# Path to storage location of processed MIMIC-III-50 data
path_proc = os.path.join(root, 'data', 'processed')


def get_train_token_freqs(model, path_dataset: str):
    """
    Function that computes or loads a dictionary containing the frequencies for each token in the training corpus.

    Parameters
    ----------
    path_dataset : str
        Path to storage location of the sequences of the training corpus

    Returns
    -------
    Dict[int, int]

    """
    from collections import Counter

    if model == 'CLF':
        file_path = os.path.join(path_dataset, 'train_tokens_freqs_trans.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train_text.pkl'))

            train_token_freqs = Counter(element for doc in list(df_train['input_ids'].tolist()) for element in doc)
            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    else:
        file_path = os.path.join(path_dataset, 'train_tokens_freqs.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train.pkl'))
            train_token_freqs = Counter(element for doc in list(df_train) for element in doc)

            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    return train_token_freqs


def truncate_seq(seq: List[int], max_doc_len: int = 3000):
    """
    Function that either truncates the clinical document sequence (seq) to the maximum length if longer or
    returns its actual length

    Parameters
    ----------
    seq : List[int]
        Unprocessed document sequence
    max_length : int; default=3000
        Maximum document length

    Returns
    -------
    List[int]

    """

    length = len(seq)

    if length > max_doc_len:
        return seq[:max_doc_len]
    return seq[:length]


def load_scores(model: str,
                att: str,
                dataset: str,
                path_results: str,
                split: str = 'test',
                seeds: List[str] = ['13', '25', '42', '87', '111'],
                max_doc_len: int = 3_000):
    """
    This function retrieves the energy scores per batch of document, contained in .hdf5 files, and matches them
    with the correct token across multiple seeds to take an average energy score value. Finally, a dictionary for each
    seed is created that contains descriptive statistics for each token w.r.t a label in the label space.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_dataset : str
        Path to directory where documents are stored
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments

    """
    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed/')
    if not os.path.exists(os.path.dirname(path_res_scores)):
        try:
            print(f'Creating directory at: {path_res_scores}')
            os.makedirs(os.path.dirname(path_res_scores))
        except OSError as error:
            print(error)

    print(f'Retrieving and consolidating energy scores for {model} and {att} label attention.')

    # Retrieve different data for Clinical Longformer (CLF); byte-pair encoded vocabulary
    if model == 'CLF':
        with open(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'truncated_seq': docs_tensor}
        docs = pd.DataFrame(data=d)
        max_doc_len = 4096
        bs = 4
    else:
        docs = pd.DataFrame(pd.read_pickle(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test.pkl')))
        docs['truncated_seq'] = docs.apply(lambda x: truncate_seq(x.token2id, max_doc_len=max_doc_len), axis=1)
        bs = 16

    # Retrieve labels for dataset
    with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        labels = pickle.load(f)

    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept.pkl'), 'rb') as f:
            labels = pickle.load(f)

    # Add code to subset label set for MimicFull to only retrieve energy scores for the relevant labels for phrase analysis
    ####
    # CODE MISSING HERE

    ####
    with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept_index.pkl'), 'rb') as f:
        mimicFull_labels_kept_idx = pickle.load(f)



    for seed in seeds:
        print(f'Current seed: {seed}')

        # Init empty dictionary for current seed
        d_tmp = {}

        # init counter variable for b indicating current batch
        b = 0

        # Use while loop to loop through files since models may have different number of batches
        # depending on batch size
        while True:
            t2 = time.process_time()
            if (b + 1) % 5 == 0:
                print(f'Current batch: {b + 1}')

            # Load hdf5 file containing energy score matrix for current batch
            file_name = f'{model}_{att}_{seed}_{split}_en_scores_batch{b}.hdf5'
            if dataset == 'MimicFull':
                file = os.path.join(path_results, f'{dataset}', f'{model}', file_name)
            else:
                file = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'analysis', file_name)
            if os.path.isfile(file):
                with h5py.File(file, 'r') as f:
                    scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]
                    scores = scores[: , mimicFull_labels_kept_idx, :]

                # retrieve sequences for current batch
                docs_batch = docs.iloc[bs * b: (b + 1) * bs]  # retrieve sequences from current batch

                # This loop aligns the energy scores to their tokenid and finally appends the score to a token-specific
                # array. This array is then used to compute descriptive statistics for a token. Appending the individual
                # scores to an array avoids averaging errors.
                for doc in range(docs_batch.shape[0]):
                    for l, label in enumerate(labels):
                        if label not in d_tmp.keys():
                            d_tmp[label] = {}
                        for t, token in enumerate(docs_batch.iloc[doc].truncated_seq):
                            if token not in d_tmp[label].keys():
                                d_tmp[label][token] = []
                            # Retrieve the label-specific (l) energy score of token t in document doc
                            d_tmp[label][token].append(scores[doc][l][t])

                b += 1
                elapsed_time2 = time.process_time() - t2
                print(f'Processing time: {elapsed_time2}')
            else:
                break

        # Take the arrays and compute descriptive statistics
        d_cons = {}
        for l, label in enumerate(labels):
            if label not in d_cons.keys():
                d_cons[label] = {}
            for t, token in enumerate(d_tmp[label].keys()):
                arr = d_tmp[label][token]
                sum_val = np.sum(arr)
                mean = np.mean(arr)
                median = np.median(arr)
                std = np.std(arr)
                min_val = np.min(arr)
                max_val = np.max(arr)
                n = len(arr)
                d_cons[label][token] = {'sum': sum_val,
                                        'mean': mean,
                                        'median': median,
                                        'std': std,
                                        'min': min_val,
                                        'max': max_val,
                                        'n': n}

        with open(os.path.join(path_res_scores,
                               f'{dataset}_{model}_{att}_{split}_{seed}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_cons, f)


def consolidate_seed_dicts(model: str,
                           att: str,
                           dataset: str,
                           path_results: str,
                           split: str = 'test',
                           seeds: List[str] = ['13', '25', '42', '87', '111']):
    """
    This function retrieves the generated seed-specific dictionaries and consolidates the contained token-speicific
    scores w.r.t its label into one general dictionary.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments

    """

    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed')

    # Retrieve labels for dataset
    with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        labels = pickle.load(f)


    dicts = []
    for seed in seeds:
        with open(os.path.join(path_res_scores,
                               f'{dataset}_{model}_{att}_{split}_{seed}_energyscores.pkl'), 'rb') as f:
            d = pickle.load(f)
        dicts.append(d)

    d_total = {}
    for label in labels:
        tokens = dicts[0][label].keys()
        d_total[label] = {}
        for token in tokens:
            sum_arr = []
            mean_arr = []
            median_arr = []
            std_arr = []
            min_arr = []
            max_arr = []
            for d_seed in dicts:
                token_dict = d_seed[label][token]
                sum_arr.append(token_dict['sum'])
                mean_arr.append(token_dict['mean'])
                median_arr.append(token_dict['median'])
                std_arr.append(token_dict['std'])
                min_arr.append(token_dict['min'])
                max_arr.append(token_dict['max'])

            d_total[label][token] = {'sum': np.mean(sum_arr),
                                     'mean': np.mean(mean_arr),
                                     'median': np.mean(median_arr),
                                     'std': np.mean(std_arr),
                                     'min': np.mean(min_arr),
                                     'max': np.mean(max_arr),
                                     'n': d_seed[label][token]['n']}

        with open(os.path.join(path_res_scores,
                               f'{dataset}_{model}_{att}_{split}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)


import argparse

parser = argparse.ArgumentParser()
# Model-unspecific commandline arguments
parser.add_argument('-m', '--model',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['CNN', 'BiLSTM', 'BiGRU', 'CLF'],
                    help='Select a predefined model.')

parser.add_argument('-am', '--attention_module',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['random', 'hierarchical_random',
                             'pretrained', 'hierarchical_pretrained'],
                    help='Select a type of predefined attention mechanism or none.')

parser.add_argument('-pre', '--preprocessing',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-d', '--dataset',
                    type=str,
                    choices=['MimicFull', 'Mimic50'])

args = parser.parse_args()


def main():
    models = args.model
    dataset = args.dataset
    atts = args.attention_module
    pre = args.preprocessing
    if dataset == 'MimicFull':
        seeds = ['25']
    else:
        seeds = ['13', '25', '42', '87', '111']
    if pre:
        for model in models:
            for att in atts:
                t = time.process_time()
                load_scores(model=model, att=att, dataset=dataset, path_results=path_results, seeds=seeds)

                consolidate_seed_dicts(model=model, att=att, path_results=path_results, dataset=dataset, seeds=seeds)
                elapsed_time = time.process_time() - t
                print(f'Processing time: {elapsed_time}')


if __name__ == '__main__':
    main()
