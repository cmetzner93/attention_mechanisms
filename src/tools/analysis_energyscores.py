"""
This file contains source code to process and analyse the energy scores generated by the attention mechanisms during
the testing of the model.
    @author: Christoph Metzner
    @email: cmetzner@vols.utk.edu
    @created: 12/09/2022
    @last modified: 12/09/2022
"""

# built-in libraries
import os
import sys
from typing import List, Dict, Tuple
import pickle
import h5py
import time
import re

# installed libraries
import numpy as np
from scipy import stats
import pandas as pd

pd.set_option('display.max_rows', None)
import torch
import matplotlib.pyplot as plt

from transformers import AutoTokenizer

CLF_tokenizer = AutoTokenizer.from_pretrained(
    "/Users/cmetzner/Desktop/Study/PhD/research/ORNL/Biostatistics and Multiscale System Modeling/attention_mechanisms/src/models/Clinical-Longformer",
    model_max_length=4096)

# Set up root folder
try:
    root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
except NameError:
    root = os.path.dirname(os.getcwd())
sys.path.append(root)
print('Project root: {}'.format(root))

# Path to storage location of results
path_results = os.path.join('/Volumes', 'MetznerSSD', 'Study_Attention')

# Path to storage location of processed MIMIC-III-50 data
path_proc = os.path.join(root, 'data', 'processed')


def get_train_token_freqs(model, path_dataset: str):
    """
    Function that computes or loads a dictionary containing the frequencies for each token in the training corpus.

    Parameters
    ----------
    path_dataset : str
        Path to storage location of the sequences of the training corpus

    Returns
    -------
    Dict[int, int]

    """
    from collections import Counter

    if model == 'CLF':
        file_path = os.path.join(path_dataset, 'train_tokens_freqs_trans.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train_text.pkl'))

            train_token_freqs = Counter(element for doc in list(df_train['input_ids'].tolist()) for element in doc)
            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    else:
        file_path = os.path.join(path_dataset, 'train_tokens_freqs.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train.pkl'))
            train_token_freqs = Counter(element for doc in list(df_train) for element in doc)

            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    return train_token_freqs


def truncate_seq(seq: List[int], max_doc_len: int = 3000):
    """
    Function that either truncates the clinical document sequence (seq) to the maximum length if longer or
    returns its actual length

    Parameters
    ----------
    seq : List[int]
        Unprocessed document sequence
    max_length : int; default=3000
        Maximum document length

    Returns
    -------
    List[int]

    """

    length = len(seq)

    if length > max_doc_len:
        return seq[:max_doc_len]
    return seq[:length]


def load_scores(model: str,
                att: str,
                dataset: str,
                path_results: str,
                split: str = 'test',
                seeds: List[str] = ['13', '25', '42', '87', '111'],
                max_doc_len: int = 3_000):
    """
    This function retrieves the energy scores per batch of document, contained in .hdf5 files, and matches them
    with the correct token across multiple seeds to take an average energy score value. Finally, a dictionary for each
    seed is created that contains descriptive statistics for each token w.r.t a label in the label space.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_dataset : str
        Path to directory where documents are stored
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments

    """
    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed/')
    if not os.path.exists(os.path.dirname(path_res_scores)):
        try:
            print(f'Creating directory at: {path_res_scores}')
            os.makedirs(os.path.dirname(path_res_scores))
        except OSError as error:
            print(error)

    print(f'Retrieving and consolidating energy scores for {model} and {att} label attention.')

    # Retrieve different data for Clinical Longformer (CLF); byte-pair encoded vocabulary
    if model == 'CLF':
        with open(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'truncated_seq': docs_tensor}
        docs = pd.DataFrame(data=d)
        max_doc_len = 4096
        bs = 4
    else:
        docs = pd.DataFrame(pd.read_pickle(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test.pkl')))
        docs['truncated_seq'] = docs.apply(lambda x: truncate_seq(x.token2id, max_doc_len=max_doc_len), axis=1)
        bs = 16

    # Retrieve labels for dataset
    with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        labels = pickle.load(f)

    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept.pkl'), 'rb') as f:
            labels = pickle.load(f)

    # Add code to subset label set for MimicFull to only retrieve energy scores for the relevant labels for phrase analysis
    ####
    # CODE MISSING HERE

    ####
    with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept_index.pkl'), 'rb') as f:
        mimicFull_labels_kept_idx = pickle.load(f)



    for seed in seeds:
        print(f'Current seed: {seed}')

        # Init empty dictionary for current seed
        d_tmp = {}

        # init counter variable for b indicating current batch
        b = 0

        # Use while loop to loop through files since models may have different number of batches
        # depending on batch size
        while True:
            t2 = time.process_time()
            if (b + 1) % 5 == 0:
                print(f'Current batch: {b + 1}')

            # Load hdf5 file containing energy score matrix for current batch
            file_name = f'{model}_{att}_{seed}_{split}_en_scores_batch{b}.hdf5'
            if dataset == 'MimicFull':
                file = os.path.join(path_results, f'{dataset}', f'{model}', file_name)
            else:
                file = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'analysis', file_name)
            if os.path.isfile(file):
                with h5py.File(file, 'r') as f:
                    scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]
                    scores = scores[: , mimicFull_labels_kept_idx, :]

                # retrieve sequences for current batch
                docs_batch = docs.iloc[bs * b: (b + 1) * bs]  # retrieve sequences from current batch

                # This loop aligns the energy scores to their tokenid and finally appends the score to a token-specific
                # array. This array is then used to compute descriptive statistics for a token. Appending the individual
                # scores to an array avoids averaging errors.
                for doc in range(docs_batch.shape[0]):
                    for l, label in enumerate(labels):
                        if label not in d_tmp.keys():
                            d_tmp[label] = {}
                        for t, token in enumerate(docs_batch.iloc[doc].truncated_seq):
                            if token not in d_tmp[label].keys():
                                d_tmp[label][token] = []
                            # Retrieve the label-specific (l) energy score of token t in document doc
                            d_tmp[label][token].append(scores[doc][l][t])

                b += 1
                elapsed_time2 = time.process_time() - t2
                print(f'Processing time: {elapsed_time2}')
            else:
                break

        # Take the arrays and compute descriptive statistics
        d_cons = {}
        for l, label in enumerate(labels):
            if label not in d_cons.keys():
                d_cons[label] = {}
            for t, token in enumerate(d_tmp[label].keys()):
                arr = d_tmp[label][token]
                sum_val = np.sum(arr)
                mean = np.mean(arr)
                median = np.median(arr)
                std = np.std(arr)
                min_val = np.min(arr)
                max_val = np.max(arr)
                n = len(arr)
                d_cons[label][token] = {'sum': sum_val,
                                        'mean': mean,
                                        'median': median,
                                        'std': std,
                                        'min': min_val,
                                        'max': max_val,
                                        'n': n}

        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_{split}_{seed}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_cons, f)


def consolidate_seed_dicts(model: str,
                           att: str,
                           dataset: str,
                           path_results: str,
                           split: str = 'test',
                           seeds: List[str] = ['13', '25', '42', '87', '111']):
    """
    This function retrieves the generated seed-specific dictionaries and consolidates the contained token-speicific
    scores w.r.t its label into one general dictionary.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments

    """

    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed')

    # Retrieve labels for dataset
    with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        labels = pickle.load(f)


    dicts = []
    for seed in seeds:
        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_{split}_{seed}_energyscores.pkl'), 'rb') as f:
            d = pickle.load(f)
        dicts.append(d)

    d_total = {}
    for label in labels:
        tokens = dicts[0][label].keys()
        d_total[label] = {}
        for token in tokens:
            sum_arr = []
            mean_arr = []
            median_arr = []
            std_arr = []
            min_arr = []
            max_arr = []
            for d_seed in dicts:
                token_dict = d_seed[label][token]
                sum_arr.append(token_dict['sum'])
                mean_arr.append(token_dict['mean'])
                median_arr.append(token_dict['median'])
                std_arr.append(token_dict['std'])
                min_arr.append(token_dict['min'])
                max_arr.append(token_dict['max'])

            d_total[label][token] = {'sum': np.mean(sum_arr),
                                     'mean': np.mean(mean_arr),
                                     'median': np.mean(median_arr),
                                     'std': np.mean(std_arr),
                                     'min': np.mean(min_arr),
                                     'max': np.mean(max_arr),
                                     'n': d_seed[label][token]['n']}

        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_{split}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)


def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=1)  # only difference


def load_energy_for_doc_label(model: str, att: str, dataset: str, k=10):
    """
    Function that retrieves the energy scores for one specific
    Parameters
    ----------
    model : str
        Type of model, e.g., CNN, BiGRU, BiLSTM, CLF
    att : str
        Type of attention mechanism, e.g., random, pretrained, hierarchical_random, hierarchical_pretrained
    Returns
    -------
p
    """
    # Load labels for MIMIC-III-50
    with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        mimic50_labels = pickle.load(f)

    # Load vocabulary for MIMIC-III-50
    with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'vocab_{dataset}.pkl'),
              'rb') as f:
        mimic50_vocab = pickle.load(f)

    # Load dictionary with energy scores consolidated on label-level
    with open(os.path.join(path_results, 'Mimic50', f'{model}', 'scores', 'energy_processed',
                           f'{model}_{att}_test_energyscores.pkl'), 'rb') as f:
        dict_scores = pickle.load(f)

    # Load HADM-IDs associated with documents in test corpus
    doc_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                       f'ids_{dataset}_test.csv')).HADM_ID.tolist()

    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'),
                  'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': doc_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        max_length = 4096
        phrase_sizes = [4, 6, 8]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = doc_ids
        max_length = 3000
        phrase_sizes = [3, 4, 5]
    start = time.time()
    k = 15
    for l, label in enumerate(mimic50_labels):
        print(f'Label: {label}')
        start = time.time()
        scores_label = dict_scores[label]
        file_name_results = f'{dataset}_{model}_{att}_top_k_phrases_{label}.csv'
        file_path =  os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction', file_name_results)

        for doc_id in doc_ids:
            start2 = time.time()
            # Retrieve integer-index document
            doc = test_docs[test_docs['hadm_id'] == doc_id]['token2id'].item()

            # Remove <pad> token and/or clip to max length
            if model == 'CLF':
                if len(doc) > max_length:
                    doc = doc[:max_length]
                else:
                    phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(doc)
                    doc_no_pad = [token for token in phrase_tokens if token != '<pad>']
                    len_doc = len(doc_no_pad)
                    doc = doc[:len_doc]
            else:
                if len(doc) > max_length:
                    doc = doc[:max_length]

            doc_len = len(doc)

            # Map aggregated energy scores to each integer-index token
            doc_energy = [scores_label[idx]['mean'] for idx in doc]

            means = []
            means_energy = []
            means_idx = []
            for ps in phrase_sizes:
                means_ps = []
                for idx in range(doc_len):
                    phrase_energy = doc_energy[idx:idx + ps]
                    means_ps.append(np.mean(phrase_energy))
                # Retrieve idx for the three phrases with the highest average score
                top_k_idx = list(np.argsort(np.array(means_ps))[::-1][:k])
                for idx in top_k_idx:
                    means.append(means_ps[idx])
                    means_energy.append(doc_energy[idx:idx + ps])
                    means_idx.append(idx)

            # Get top 3 means across the phrase sizes for each label
            top_k_idx = list(np.argsort(np.array(means))[::-1][:k])

            # Add means to array that contains the means across the 3 phrase sizes
            for idx in top_k_idx:
                means.append(means[idx])
                means_energy.append(means_energy[idx])
                means_idx.append(means_idx[idx])

            # Remove duplicate phrases
            means_idx_no_duplicates = []
            means_no_duplicates = []
            means_energy_no_duplicates = []
            for i, idx in enumerate(means_idx):
                if idx not in means_idx_no_duplicates:
                    means_idx_no_duplicates.append(idx)
                    means_no_duplicates.append(means[i])
                    means_energy_no_duplicates.append(means_energy[i])

            # Loop to remove direct neighbors
            #means_idx_old = means_idx_no_duplicates.copy()
            #for i, idi in enumerate(means_idx_no_duplicates):
            #    ub = idi + 2
            #    lb = idi - 2
            #    for j, idj in enumerate(means_idx_no_duplicates[:]):
            #        if i != j:
            #            if ub >= idj >= lb:
            #                means_idx_no_duplicates.remove(idj)  # this requires the duplicate removal process

            #means_no_duplicates = [means_no_duplicates[means_idx_old.index(idx)] for idx in means_idx_no_duplicates]
            #means_energy_no_duplicates = [means_energy_no_duplicates[means_idx_old.index(idx)] for idx in
            #                              means_idx_no_duplicates]

            # Get top 3 means across the phrase sizes for each label
            k = 5
            top_k_ids = list(np.argsort(np.array(means_no_duplicates))[::-1][:k])
            means = []
            means_energy = []
            means_idx = []
            for idx in top_k_ids:
                means.append(means_no_duplicates[idx])
                means_energy.append(means_energy_no_duplicates[idx])
                means_idx.append(means_idx_no_duplicates[idx])

            while len(means) < 5:
                means.append(0)
                means_energy.append([0])
                means_idx.append(0)

            top_k_phrases = []
            for mean, mean_energy, mean_idx in zip(means, means_energy, means_idx):
                top_k_phrases.append((mean, mean_energy, mean_idx))

            # Init helper dictionary that is casted to pandas csv
            d_helper = {'model': [model] * k, 'att': [att] * k, 'doc_id': [doc_id] * k, 'rank': np.arange(1, k+1),
                        'phrase': [], 'phrase_tokens': [], 'energy_mean': [], 'energy_token': [], 'phrase_pos_idx': []}
            for top_phrase in top_k_phrases:
                mean_score = top_phrase[0]
                phrase_scores = top_phrase[1]
                start_pos = top_phrase[2]

                phrase_idx = doc[start_pos: start_pos + len(phrase_scores)]
                # Decode sequence
                if model == 'CLF':
                    #  phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
                    phrase_tokens = CLF_tokenizer.decode(phrase_idx)
                else:
                    phrase_tokens = mimic50_vocab.lookup_tokens(phrase_idx)

                d_helper['phrase'].append(" ".join(phrase_tokens))
                d_helper['phrase_tokens'].append(phrase_tokens)
                d_helper['energy_mean'].append(mean_score)
                d_helper['energy_token'].append(phrase_scores)
                d_helper['phrase_pos_idx'].append(start_pos)

            df = pd.DataFrame(d_helper)
            if os.path.isfile(file_path):
                df.to_csv(file_path, mode='a', header=False)
            else:
                df.to_csv(file_path, header=d_helper.keys())

        df_label = pd.read_csv(file_path).reset_index(drop=True)
        df_label.to_csv(file_path)

        print(f'Execution time: {time.time() - start}')


def get_phrase_frequency(model, att, dataset):
    # Load labels for MIMIC-III-50
    with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        mimic50_labels = pickle.load(f)

    # Load category descriptions
    code_descs = pd.read_pickle(os.path.join(root, 'data', 'processed', 'cleaned_descriptions.pkl')).reset_index(
        drop=True)
    code_descs = code_descs.drop(['desc_clean', 'desc_len'], axis=1)
    df_mimic50_desc = code_descs[code_descs['ICD9_CODE'].isin(mimic50_labels)].reset_index(drop=True)
    df_mimic50_desc.set_index('ICD9_CODE', inplace=True)

    mimic50_desc = df_mimic50_desc.to_dict()['LONG_TITLE']

    d = {'model': [], 'att': [], 'ICD9_code': [], 'ICD9_code_desc': [], 'rank': [], 'phrase': [], 'count': []}

    for l, label in enumerate(mimic50_labels):
        print(f'Label: {label}')
        file_name_results = f'{dataset}_{model}_{att}_top_k_phrases_{label}.csv'
        file_path = os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction', file_name_results)
        df = pd.read_csv(file_path).reset_index(drop=True)
        df = df.drop(['Unnamed: 0'], axis=1)

        phrase_counts = df.phrase.value_counts()[:5]

        for count, phrase in enumerate(phrase_counts.index.tolist()):
            if model == 'CLF':
                # remove spaces after each character
                phrase = ' '.join([re.sub(' ', '', token) for token in phrase.split('  ')]).strip()
            d['ICD9_code'].append(label)
            d['ICD9_code_desc'].append(mimic50_desc[label])
            d['model'].append(model)
            d['att'].append(att)
            d['rank'].append(count+1)
            d['phrase'].append(phrase)
            d['count'].append(phrase_counts[count])

    df = pd.DataFrame(d)

    df.to_csv(os.path.join(path_results, f'{dataset}', f'{dataset}_{model}_{att}_top_k_phrases.csv'),
              header=True, index=True)


def get_phrases_for_doc(top_k_phrases, model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 42):
    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 4096
        #phrase_sizes = [6, 8, 10]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 3000
        #test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
        #phrase_sizes = [3, 4, 5]

    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()

    phrases = []
    for top_phrase in top_k_phrases:
        mean_score = top_phrase[0]
        phrase_scores = top_phrase[1]
        len_seq = phrase_scores.shape[0]
        start_pos = top_phrase[2]

        phrase_idx = seq[start_pos: start_pos + len_seq]
        # Decode sequence
        if model == 'CLF':
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
        else:
            with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'vocab_{dataset}.pkl'), 'rb') as f:
                vocab = pickle.load(f)
            phrase_tokens = vocab.lookup_tokens(phrase_idx)
        phrases.append((phrase_tokens, mean_score, phrase_scores, start_pos))

    print(f'{model}/{att}: {phrases}')
    return phrases


def load_energy_for_doc(model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 25):
    """
    Function that retrieves the energy scores for one specific
    Parameters
    ----------
    model : str
        Type of model, e.g., CNN, BiGRU, BiLSTM, CLF
    att : str
        Type of attention mechanism, e.g., random, pretrained, hierarchical_random, hierarchical_pretrained
    Returns
    -------

    """
    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept_index.pkl'), 'rb') as f:
            labels = pickle.load(f)
    elif dataset == 'Mimic50':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
            labels = pickle.load(f)


    # Load testids for dataset
    test_ids = pd.read_csv(
        os.path.join(root, 'data', 'processed', f'data_{dataset}', f'ids_{dataset}_test.csv')).HADM_ID.tolist()

    # We want to match the phrases highlighted by a medical resident with the phrases extracted from our models
    # Given that the energy score file size is so large we are not processing the energy scores to get an
    # aggregated value for each token --> we only care about the energy scores for a given document
    # To do that, we need to extract the specific location of each document in the ordered test dataset and
    # therefore, locate them in each of the batches

    # First: Retrieve absolute position of each document in doc_hadm_ids_marked in test dataset
    #doc_hadm_ids = [119205, 112942, 163484, 136507, 131550, 123767, 112686, 182238, 116467]
    doc_idx = [test_ids.index(idx) for idx in doc_hadm_ids]

    # Third: Retrieve the batch number and the specific doc number in the batch
    if model == 'CLF':
        batch_size = 4  # batch_size
        phrase_sizes = [3, 4, 5]
    else:
        batch_size = 16  # batch_size
        phrase_sizes = [3, 4, 5]

    # Energyscores are in same order as test documents during testing
    # Energyscores of each batch are stored separately
    batch_vals = []
    for hadm_id, doc_id in zip(doc_hadm_ids, doc_idx):
        b = doc_id // batch_size
        doc_idx_in_batch = doc_id % batch_size - 1
        if doc_idx_in_batch == -1:
            doc_idx_in_batch = 0
        #print(f'{hadm_id}/{doc_id}: bn(doc_pos): {b}({doc_idx_in_batch})')
        batch_vals.append((b, doc_idx_in_batch))

    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 4096
        phrase_sizes = [4, 6, 8]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
        phrase_sizes = [3, 4, 5]

    # Check length of sequence
    # Get sequence
    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()
    # Need to remove <pad>
    seq_length = len(seq)
    if model == 'CLF':
        if len(seq) > 4096:
            seq = seq[:4096]
        else:
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(seq)
            seq = [token for token in phrase_tokens if token != '<pad>']
    else:
        if len(seq) > 3000:
            seq = seq[:3000]
    seq_length = len(seq)
    # Load energy score file
    for b, doc_idx_in_batch in batch_vals:
        file_name = f'{model}_{att}_{seed}_test_en_scores_batch{b}.hdf5'
        if dataset == 'MimicFull':
            file = os.path.join(path_results, f'{dataset}', f'{model}', file_name)
        else:
            file = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'analysis', file_name)
        if os.path.isfile(file):
            with h5py.File(file, 'r') as f:
                scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]
                if dataset == 'MimicFull':
                    scores = scores[doc_idx_in_batch, labels, :]
                else:
                    scores = scores[doc_idx_in_batch, :, :]
        #scores = softmax(scores)

        # Extract phrases with largest average energy scores for 3, 4, 5 and 4, 6, 8 tokens
        # 1. Loop to extract key phrases for different window sizes across all considered labels

        # Loop through energy score matrix
        k = 10
        means = []
        means_energy = []
        means_idx = []
        for l, label in enumerate(labels):
            scores_label = scores[l]

            label_means = []
            label_means_energy = []
            label_means_idx = []
            # Get subsequences from sequences for 3 (4), 4 (6), and 5 (8) tokens for CNN/RNN (CLF).
            for ps in phrase_sizes:
                means_ps = []
                # Loop through sequence
                for idx in range(seq_length):
                    phrase_energy = scores_label[idx:idx+ps]
                    means_ps.append(np.mean(phrase_energy))



                # Retrieve idx for the three phrases with the highest average score
                top_k_idx = list(np.argsort(np.array(means_ps))[::-1][:k])

                for idx in top_k_idx:
                    label_means.append(means_ps[idx])
                    label_means_energy.append(scores_label[idx:idx+ps])
                    label_means_idx.append(idx)


            #print(label_means)
            #print(label_means_energy)
            #print(label_means_idx)


            # Get top 3 means across the phrase sizes for each label
            top_k_idx = list(np.argsort(np.array(label_means))[::-1][:k])

            # Add means to array that contains the means across the 3 phrase sizes
            for idx in top_k_idx:
                means.append(label_means[idx])
                means_energy.append(label_means_energy[idx])
                means_idx.append(label_means_idx[idx])

        # Remove duplicate phrases
        means_idx_no_duplicates = []
        means_no_duplicates = []
        means_energy_no_duplicates = []
        for i, idx in enumerate(means_idx):
            if idx not in means_idx_no_duplicates:
                means_idx_no_duplicates.append(idx)
                means_no_duplicates.append(means[i])
                means_energy_no_duplicates.append(means_energy[i])
        # Loop to remove direct neighbors
        means_idx_old = means_idx_no_duplicates.copy()
        for i, idi in enumerate(means_idx_no_duplicates):
            ub = idi + 2
            lb = idi - 2
            for j, idj in enumerate(means_idx_no_duplicates[:]):
                if i != j:
                    if ub >= idj >= lb:
                        means_idx_no_duplicates.remove(idj)  # this requires the duplicate removal process

        means_no_duplicates = [means_no_duplicates[means_idx_old.index(idx)] for idx in means_idx_no_duplicates]
        means_energy_no_duplicates = [means_energy_no_duplicates[means_idx_old.index(idx)] for idx in means_idx_no_duplicates]

        # Get top 3 means across the phrase sizes for each label
        top_k_ids = list(np.argsort(np.array(means_no_duplicates))[::-1][:k])
        means = []
        means_energy = []
        means_idx = []
        for idx in top_k_ids:
            means.append(means_no_duplicates[idx])
            means_energy.append(means_energy_no_duplicates[idx])
            means_idx.append(means_idx_no_duplicates[idx])

        top_k_phrases = []
        for mean, mean_energy, mean_idx in zip(means, means_energy, means_idx):
            top_k_phrases.append((mean, mean_energy, mean_idx))

        return top_k_phrases


def get_phrases_for_doc(top_k_phrases, model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 42):
    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 4096
        #phrase_sizes = [6, 8, 10]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 3000
        #test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
        #phrase_sizes = [3, 4, 5]

    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()

    phrases = []
    for top_phrase in top_k_phrases:
        mean_score = top_phrase[0]
        phrase_scores = top_phrase[1]
        len_seq = phrase_scores.shape[0]
        start_pos = top_phrase[2]

        phrase_idx = seq[start_pos: start_pos + len_seq]
        # Decode sequence
        if model == 'CLF':
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
        else:
            with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'vocab_{dataset}.pkl'), 'rb') as f:
                vocab = pickle.load(f)
            phrase_tokens = vocab.lookup_tokens(phrase_idx)
        phrases.append((phrase_tokens, mean_score, phrase_scores, start_pos))

    print(f'{model}/{att}: {phrases}')
    return phrases


def get_token_ids(model: str, att: str, split: str) -> List[int]:
    """
    Helper function that extract the list of token indices contained in the energy score dictionaries

    Parameters
    ----------
    model : str
        Model name, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Name of attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_pretrained |
    split : str
        Dataset split

    Returns
    -------
    List[int]
        List containing the token indices

    """

    if model == 'CLF':
        if os.path.isfile(os.path.join(path_results, f'token_ids_{split}_text.pkl')):
            with open(os.path.join(path_results, f'token_ids_{split}_text.pkl'), 'rb') as f:
                token_ids = pickle.load(f)
        else:
            with open(os.path.join(path_results, f'results_analysis_{model}', 'scores',
                                   f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)
            token_ids = list(d_scores[list(d_scores.keys())[0]].keys())
            with open(os.path.join(path_results, f'token_ids_{split}_text.pkl'), 'wb') as f:
                pickle.dump(token_ids, f)
    else:
        if os.path.isfile(os.path.join(path_results, f'token_ids_{split}.pkl')):
            with open(os.path.join(path_results, f'token_ids_{split}.pkl'), 'rb') as f:
                token_ids = pickle.load(f)
        else:
            with open(os.path.join(path_results, f'results_analysis_{model}', 'scores',
                                   f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)

            token_ids = list(d_scores[list(d_scores.keys())[0]].keys())
            with open(os.path.join(path_results, f'token_ids_{split}.pkl'), 'wb') as f:
                pickle.dump(token_ids, f)

    return token_ids


def get_k_largest_scores(model: List[str],
                         att: List[str],
                         train_token_freqs: Dict[int, int],
                         split: str = 'test',
                         min_words: int = 3,
                         k: int = 3,
                         stat: str = 'mean'):
    """
    This function matches the energy scores to its tokens in a document for multiple seeds

    """
    path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

    # load list of tokens in dataset
    token_ids = get_token_ids(model=model, att=att, split=split)

    scores = {}
    # Load the created dictionary
    with open(os.path.join(path_res_energy,
                           f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
        dict_scores = pickle.load(f)

    for l, label in enumerate(dict_scores):
        stat_tmp = []
        tokens = []
        tokens_tmp = []

        for token in token_ids:
            if dict_scores[label][token]['n'] > min_words:
                stat_tmp.append(dict_scores[label][token][stat])
                tokens.append(token)
                tokens_tmp.append(dict_scores[label][token])

        stat_tmp_arr = np.array(stat_tmp)
        tokens_arr = np.array(tokens)
        tokens_tmp_arr = np.array(tokens_tmp)

        idx_stat_max = np.argsort(stat_tmp_arr)[:-k - 1:-1]
        tokens_arr_max = tokens_arr[idx_stat_max]
        tokens_tmp_arr_max = tokens_tmp_arr[idx_stat_max]

        # add vocab lookup from vocab to add word
        # token2id and id2token
        for token_id, token_d in zip(tokens_arr_max, tokens_tmp_arr_max):
            if model == 'CLF':
                token = CLF_tokenizer.convert_ids_to_tokens([token_id])[0]
            else:
                token = mimic50_vocab.lookup_token(token_id)
            token_d['token'] = token
            token_d['id'] = token_id
            token_d['train_n'] = train_token_freqs[token_id]

        # top_words = [dict_scores[label][token] for token in tokens_arr_max]
        scores[label] = tokens_tmp_arr_max

    return scores


def create_dfs_from_dict(d: Dict[str, float], k: int, min_words: int):
    # Create multiindex and multiindex columns for dataframe
    token_features = ['token', 'mean', 'std', 'n', 'train_n']  # features in lower col index
    lower_col_index = token_features * k  # have to multiple features for k tokens
    upper_col_index = []
    for i in range(k):
        tokens = [f'token_{i + 1}'] * len(token_features)
        upper_col_index += tokens
    col_index = [upper_col_index, lower_col_index]
    row_index = [list(mimic50_desc.label),
                 list(mimic50_desc.description),
                 list(mimic50_desc.frequency),
                 list(mimic50_desc.quartile)]

    # Create one nested dictionary that contains the tokens with the highest energy scores
    dfs = {}

    # Loop through all the considered models (i.e., text encoder architectures)
    for model in d:
        dfs[model] = {}

        # Loop through all attention mechanisms
        for att in d[model]:
            model_att = []
            for label in d[model][att]:
                label_arr = []
                for token_d in d[model][att][label]:
                    for feature in token_features:
                        label_arr.append(token_d[feature])
                model_att.append(label_arr)

            model_att_arr = np.array(model_att)
            df = pd.DataFrame(model_att_arr, index=row_index, columns=col_index)

            dfs[model][att] = df

    with open(os.path.join(path_results, 'tokens_phrases', f'token_energy_dict_mw{min_words}.pkl'), 'wb') as f:
        pickle.dump(dfs, f)


def get_k_tokens(models: List[str], atts: List[str], labels: List[str], min_words: int):
    if labels == ['all']:
        labels = mimic50_labels
    # Load k largest tokens per label for each model attention mechanism
    with open(os.path.join(path_results, 'tokens_phrases', f'token_energy_dict_mw{min_words}.pkl'), 'rb') as f:
        d_energy = pickle.load(f)

    tokens_all = []
    for model in models:
        for att in atts:
            tokens = d_energy[model][att].loc[labels, :].reset_index()
            tokens = tokens.rename(columns={'level_0': 'label',
                                            'level_1': 'desc',
                                            'level_2': 'freq',
                                            'level_3': 'quartile'})
            tokens.insert(0, 'model', [model] * len(labels))
            tokens.insert(1, 'att', [att] * len(labels))

            tokens_all.append(tokens)

    df = pd.concat(tokens_all).reset_index(drop=True)

    df.to_excel(os.path.join(path_results, 'tokens_phrases', f'Mimic50_largest_tokens_scores_mw{min_words}.xlsx'))


def check_label_in_Y(Y, label):
    if label in Y:
        return 1
    return 0


def load_prediction_files(models: List[str], atts: List[str], n_docs: int, seeds: List[str] = [13, 25, 42, 87, 111]):
    """

    Parameters
    ----------
    models : List[str]
        Model names
    atts : List[str]
        Attention mechanisms
    n_docs : int
        Number of documents in the dataset
    seeds : List[int]; default [13, 25, 42, 87, 111]
        List of seed numbers

    Returns
    -------
    Dict[str, Dict[str, Dict[str, np.array]]]

    """

    if os.path.isfile(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl')):
        with open(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl'), 'rb') as f:
            predictions_dict = pickle.load(f)
    else:
        params = {'CNN': '100_100_16_0.5_False_None',
                  'BiGRU': '256_100_16_0.5_None_None',
                  'BiLSTM': '512_200_16_0.5_None_None',
                  'CLF': '768_300_4_0.0_False_None'}

        predictions_dict = {}
        for model in models:
            predictions_dict[model] = {}
            for att in atts:
                predictions_dict[att] = {}
                preds_matrix = np.zeros((len(seeds), n_docs, len(mimic50_labels)), dtype=int)
                probs_matrix = np.zeros((len(seeds), n_docs, len(mimic50_labels)), dtype=float)
                for s, seed in enumerate(seeds):
                    # Retrieve predictions
                    file_name = f'{model}_Mimic50_{att}_{params[model]}_{seed}_test_preds.txt'
                    path_to_file = os.path.join(path_results, f'results_analysis_{model}', 'predictions', file_name)
                    with open(path_to_file, 'r') as f:
                        docs_preds = f.readlines()

                    # Retrieve probabilities
                    file_name = f'{model}_Mimic50_{att}_{params[model]}_{seed}_test_probs.txt'
                    path_to_file = os.path.join(path_results, f'results_analysis_{model}', 'predictions', file_name)
                    with open(path_to_file, 'r') as f:
                        docs_probs = f.readlines()

                    hadm_ids = []
                    for d, (doc_preds, doc_probs) in enumerate(zip(docs_preds, docs_probs)):

                        # deal with predictions
                        doc_preds = doc_preds.strip()
                        doc_preds_split = doc_preds.split('|')
                        hadm_id = doc_preds_split[0]
                        hadm_ids.append(hadm_id)
                        doc_preds = doc_preds_split[1:]

                        # deal with probabilities
                        doc_probs = doc_probs.strip()
                        doc_probs_split = doc_probs.split('|')
                        doc_probs = np.array(doc_probs_split[1:])

                        for l, label in enumerate(mimic50_labels):
                            probs_matrix[s][d] = doc_probs
                            for pred in doc_preds:
                                if label == pred:
                                    preds_matrix[s][d][l] = 1

                preds_matrix = np.round(np.mean(preds_matrix, axis=0))
                probs_matrix = np.median(probs_matrix, axis=0)
                predictions_dict[model][att] = {'hadm_ids': hadm_ids, 'preds': preds_matrix, 'probs': probs_matrix}
        with open(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl'), 'wb') as f:
            pickle.dump(predictions_dict, f)

    return predictions_dict


def retrieve_docs_with_labels(models, atts, path_dataset, labels: List[str], n_labels: int = None):
    """
    This function retrieves documents with ground-truth labels for all labels.

    Parameters
    ----------
    models : List[str]
        Model names
    atts : List[str]
        Attention mechanisms
    labels : List[str]
        List with labels

    Return
    ------
    List[str]
        List containing hadm_ids for each document associated with all labels

    """
    # Load raw dataset which contains ground truth labels for each processed hospital admission ID
    df_50 = pd.read_pickle(os.path.join(path_dataset, 'DATA_Mimic50.pkl'))

    # Load hospital admission IDs (HADM_ID) associated with test dataset
    test_HADM_IDs = pd.read_csv(os.path.join(path_dataset, 'ids_Mimic50_test.csv')).HADM_ID.tolist()
    predictions_dict = load_prediction_files(models=models, atts=atts, n_docs=len(test_HADM_IDs))

    # Subset raw data to only contain test samples
    df_test = df_50[df_50['HADM_ID'].isin(test_HADM_IDs)]
    print(df_test.head(3))
    df_test_shape = df_test.shape
    print(f'Shape of dataset: {df_test_shape}')
    # remove documents that have more than specified number of ground-truth labels
    if n_labels is not None:
        df_test['n_labels'] = df_test.apply(lambda x: len(x['ICD9_CODE']), axis=1)
        df_test = df_test[df_test['n_labels'] <= n_labels]
        print(
            f'Setting maximum number of ground-truth labels to {n_labels} removed {df_test_shape[0] - df_test.shape[0]} documents')
        df_test_shape = df_test.shape
        print(f'Shape of reduced dataset: {df_test_shape}')

    # Remove documents that do not possess selected labels
    df_test['contains_labels'] = df_test.apply(lambda x: all(item in x.ICD9_CODE for item in labels), axis=1)
    df_test = df_test[df_test['contains_labels'] == True]
    print(
        f'Removing documents that do not contain the labels {labels} removed {df_test_shape[0] - df_test.shape[0]}')
    df_test_shape = df_test.shape
    print(f'Shape of reduced dataset: {df_test_shape}')
    doc_ids = df_test.HADM_ID.tolist()
    doc_nlabels = df_test.n_labels.tolist()
    print(doc_ids)

    # Check if documents have correctly predicted labels
    df_test['label_indices'] = df_test.apply(lambda x: [mimic50_labels.index(code) for code in x['ICD9_CODE']], axis=1)
    docs_label_indices = df_test['label_indices'].tolist()
    d = {str(doc_id): [] for doc_id in doc_ids}
    for model in models:
        for att in atts:
            # Need to retrieve information about predictions / probabilities for documents in question
            docs_indices = [predictions_dict[model][att]['hadm_ids'].index(str(hadm_id)) for hadm_id in doc_ids]
            docs_preds = predictions_dict[model][att]['preds'][docs_indices]
            docs_probs = predictions_dict[model][att]['probs'][docs_indices]

            for doc_id, doc_label_indices, doc_preds, doc_probs in zip(doc_ids, docs_label_indices, docs_preds, docs_probs):
                doc_preds = doc_preds[doc_label_indices]
                d[str(doc_id)].append(np.sum(doc_preds))
    sums = []
    for (doc_id, preds), n_labels in zip(d.items(), doc_nlabels):
        sums.append(sum(preds)/n_labels)

    max_idx = np.argmax(sums)
    doc_id = doc_ids[max_idx]
    doc_id = np.random.choice(doc_ids, 1)[0]
    return doc_ids


def id2energy(energy_scores: Dict[str, Dict[str, float]], seq: List[int]):
    return [energy_scores[idx]['mean'] for idx in seq]


def get_top_k_phrases(labels: List[str],
                      energy_scores: Dict[str, Dict[str, float]],
                      window_sizes: List[int],
                      seq: List[int]) -> List[Tuple[int, List[float]]]:

    """
    Function that extract the top k key phrases (i.e., phrases with the largest average energy scores) for a given
    document sequence. The key phrases can be 3, 4, or 5 tokens long for CNN and RNNs. Since CLFs are looking at sub-
    word tokens we allow the model extract 4, 6, or 8 token-long phrases. Since we look at differently long phrases,
    the program could extract duplicate phrases (i.e., the same phrase with one or more tokens); therefore, to allow
    more diversity in the selected key phrases we remove key phrases that are within 3 tokens of the next highest phrase

    Parameters
    ----------
    labels : List[str]
        List of ground-truth labels associated
    energy_scores : Dict[str, Dict[str, float]]
        Dictionary that contains the energy scores for each label; each label has different energy scores associated
        with each token
    window_sizes : List[int]
        Number of tokens looked at the same time
    seq : List[int]
        Original integer-indexed document sequence

    Returns
    -------
    List[Tuple[int, List[float]]]

    """

    # 1. Loop to extract key phrases for different window sizes across all considered labels

    means = []
    phrases = []
    ids = []
    for label in labels:
        seq_energy = id2energy(energy_scores=energy_scores[label], seq=seq)
        len_seq = len(seq_energy)
        for i, ws in enumerate(window_sizes):
            for idx in range(len_seq):
                phrase = seq_energy[idx:idx + ws]
                phrases.append(phrase)
                means.append(np.mean(phrase))
                ids.append(idx)

    # Extract top 15 phrases for the document and labels - np.argsort(retrieves the indices of the 15 largest mean
    # energy scores
    top_phrases_ids = list(np.argsort(np.array(means))[::-1][:50])

    ### The following code block removes duplicate phrases, i.e., phrases that start at the same index in the document
    # Assign each extract index position to retrieve the actual index position in the document
    top_ids = [ids[idx] for idx in top_phrases_ids]
    # The following loops ensures that we extract at least 3 unique key phrases

    # This loop removes duplicate indices but retains the phrase with the largest energy energy score
    top_k_ids = []
    for elem in top_ids:
        if elem not in top_k_ids:
            top_k_ids.append(elem)

    # This loop removes phrases that are very close to each other and do not provide new insight
    for i, idi in enumerate(top_k_ids):
        ub = idi + 3
        lb = idi - 3
        for j, idj in enumerate(top_k_ids[:]):
            if i != j:
                if ub >= idj >= lb:
                    top_k_ids.remove(idj)  # this requires the duplicate removal process

    top_k_ids = top_k_ids[:6]  # Make sure we have max 3 phrases

    # This just helps to retrieve the appropriate phrases
    helper_list = []
    for idx in top_k_ids:
        for i, idx_og in enumerate(top_ids):
            if idx == idx_og:
                helper_list.append(i)
                break
    top_k_phrases_ids = [top_phrases_ids[idx] for idx in helper_list]

    # Retrieve phrases
    top_k_phrases = [(idx, phrases[i]) for idx, i in zip(top_k_ids, top_k_phrases_ids)]
    return top_k_phrases


def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)  # only difference


def get_phrase(model: str, seq: List[int], starting_idx_of_max_phrase: int, ps: int = 5):
    """
    Function that retrieves the tokens (words) for max phrase.

    Parameters
    ----------
    model : str
        Name of model
    seq : List[float]
        List with energy scores, where each element is an energy score for a specific token
    starting_idx_of_max_phrase : int
        Index of first token to indicate beginning of phrase
    ps : int; default=5
        Window size of phrase, e.g., how many tokens should be included

    Returns
    -------
    List[str]

    """

    # Get token indices from max phrase
    phrase_idx = seq[starting_idx_of_max_phrase: starting_idx_of_max_phrase + ps]

    # Look up tokens from vocabulary loaded in section 1.2
    if model == 'CLF':
        phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
    else:
        phrase_tokens = mimic50_vocab.lookup_tokens(phrase_idx)
    return phrase_tokens


def get_context_of_phrase(model: str, seq: List[int], starting_idx_of_max_phrase: int, ps: int = 5, ws: int = 5):
    """
    Function that retrieves the tokens (words) for max phrase.

    Parameters
    ----------
    model : str
        Name of model
    seq : List[float]
        List with energy scores, where each element is an energy score for a specific token
    starting_idx_of_max_phrase : int
        Index of first token to indicate beginning of phrase
    ps : int; default=5
        Window size of phrase, e.g., how many tokens should be included

    Returns
    -------
    List[str]

    """

    # Get token indices from max phrase
    phrase_idx = seq[starting_idx_of_max_phrase - ws: starting_idx_of_max_phrase + ps + ws]

    if model == 'CLF':
        phrase_tokens = CLF_tokenizer.decode(phrase_idx)
    else:
        # Look up tokens from vocabulary loaded in section 1.2
        phrase_tokens = ' '.join(mimic50_vocab.lookup_tokens(phrase_idx))
    return phrase_tokens


def get_phrases_with_largest_score(models: List[str],
                                   atts: List[str],
                                   labels: str,
                                   split: str = 'test',
                                   doc_ids: List[int] = None,
                                   hier_cat_context_K: bool = False,
                                   hier_query_label_K: bool = False,
                                   get_att_scores: bool = False):
    """
    Function that retrieves the phrase of size ps with largest average energy score per document in dataset.

    Parameters
    ----------
    models : List[str]
        List of models of interest; e.g., ['CNN', 'BiGRU', 'BiLSTM', 'CLF']
    atts : List[str]
        List of attention mechanisms of interest;
        e.g., ['random', 'pretrained', 'hierarchical_random', 'hierarchical_pretrained']
    label : str
        Medical code of interest
    split : str; default='test'
        Dataset split, e.g., 'test', 'val'
    doc_ids : List[int]; default=None
        List containing dataframe indices for documents with ground truth labels associated with given labels to subset
        the dataset

    """
    test_ids = pd.read_csv(os.path.join(path_mimic50, 'ids_Mimic50_test.csv')).HADM_ID.tolist()
    predictions_dict = load_prediction_files(models=models, atts=atts, n_docs=len(test_ids))

    df_50 = pd.read_pickle(os.path.join(path_mimic50, 'DATA_Mimic50.pkl'))
    df_test = df_50[df_50['HADM_ID'].isin(doc_ids)]
    labels = df_test.ICD9_CODE.tolist()[0]


    # Loop through all models and attention mechanisms that were specified
    data = []
    for model in models:
        path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')
        for att in atts:
            doc_pred_id = predictions_dict[model][att]['hadm_ids'].index(str(doc_ids[0]))
            preds = predictions_dict[model][att]['preds'][doc_pred_id]
            probs = predictions_dict[model][att]['probs'][doc_pred_id]

            pred_labels = []
            for i, (pred, prob) in enumerate(zip(preds, probs)):
                if pred == 1:
                    pred_labels.append(mimic50_labels[i])

            prob_y_labels = []
            for label in labels:
                idx = mimic50_labels.index(label)
                prob_y_labels.append(probs[idx])

            prob_yhat_labels = []
            for pred_label in pred_labels:
                idx = mimic50_labels.index(pred_label)
                prob_yhat_labels.append(probs[idx])

            # Loading dictionary containing energy scores for each model and label attention combination
            with open(os.path.join(path_res_energy, f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                dict_scores = pickle.load(f)

            print(f'Extracting top k phrases for {model} with {att} label attention: ')
            # Sequences are different for the CNN,RNNs and the CLF
            if model == 'CLF':
                with open(os.path.join(path_mimic50, f'X_Mimic50_test_text.pkl'), 'rb') as f:
                    docs_tensor = pickle.load(f)['input_ids'].tolist()
                d = {'hadm_id': test_ids, 'token2id_max': docs_tensor}
                test_docs = pd.DataFrame(data=d)
                test_docs = test_docs[test_docs['hadm_id'].isin(doc_ids)]
                doc_length = 4096
                phrase_sizes = [6, 8, 10]

            else:  # CNN and BiGRU / BiLSTM data

                test_docs = pd.DataFrame(
                    pd.read_pickle(os.path.join(path_mimic50, f'X_Mimic50_test.pkl')))
                test_docs['hadm_id'] = test_ids

                test_docs = test_docs[test_docs['hadm_id'].isin(doc_ids)]
                doc_length = 3000
                test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
                phrase_sizes = [3, 4, 5]

            seq = test_docs[test_docs['hadm_id'] == doc_ids]['token2id_max'].item()

            top_k_phrases = get_top_k_phrases(labels=labels,
                                              energy_scores=dict_scores,
                                              window_sizes=phrase_sizes,
                                              seq=seq)
            top_k_phrases_decoded = []
            for idx, phrase in top_k_phrases:
                phrase_decoded = get_phrase(model=model, seq=seq, starting_idx_of_max_phrase=idx, ps=len(phrase))
                top_k_phrases_decoded.append(phrase_decoded)


            data.append([model, att, doc_ids[0], labels, prob_y_labels, pred_labels, prob_yhat_labels])
            columns = ['model', 'att_module', 'hadm_id', 'y_labels', 'y_probs', 'yhat_labels', 'yhat_probs']

            for i, (energy_phrase, phrase) in enumerate(zip(top_k_phrases, top_k_phrases_decoded)):
                data[-1].append(energy_phrase)
                data[-1].append(phrase)
                columns.append(f'{i+1}_phrase_scores')
                columns.append(f'{i+1}_phrase_tokens')


    df = pd.DataFrame(data=data,
                      columns=columns)
    df.to_excel(
        os.path.join(path_results, 'tokens_phrases', f'Mimic50_max_phrases_{doc_ids[0]}.xlsx'))
    return df


def compute_energyscores_statistics(models: str,
                                    atts: str,
                                    min_words: int = 3):
    stats_label = []
    stats_modelatt = []
    for model in models:
        path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

        for att in atts:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_test_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)
            vals_modelatt = []
            for label in d_scores.keys():
                vals = []
                for token in d_scores[label].keys():
                    if d_scores[label][token]['n'] >= min_words:
                        vals.append(d_scores[label][token]['mean'])
                        vals_modelatt.append(d_scores[label][token]['mean'])
                vals = np.array(vals, dtype=np.float64)
                summation = np.sum(np.array(list(map(abs, vals)), dtype=np.float64))
                mean = np.mean(vals)
                std = np.std(vals)
                min = np.min(vals)
                max = np.max(vals)
                CV = std / mean
                median = np.median(vals)
                Q25 = np.quantile(vals, 0.25)
                Q75 = np.quantile(vals, 0.75)
                n_pos_per = np.sum(vals >= 0) / len(vals)
                n_neg_per = np.sum(vals < 0) / len(vals)
                elements = [model, att, label,
                            np.round(summation, 3), np.round(mean, 3), np.round(std, 3),
                            np.round(CV, 3), np.round(min, 3), np.round(max, 3),
                            np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                            np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
                stats_label.append(elements)

            vals_modelatt = np.array(vals_modelatt, dtype=np.float64)
            summation = np.sum(np.array(list(map(abs, vals_modelatt)), dtype=np.float64))
            mean = np.mean(vals_modelatt)
            std = np.std(vals_modelatt)
            min = np.min(vals_modelatt)
            max = np.max(vals_modelatt)
            CV = abs(std) / abs(mean)
            median = np.median(vals_modelatt)
            Q25 = np.quantile(vals_modelatt, 0.25)
            Q75 = np.quantile(vals_modelatt, 0.75)
            n_pos_per = np.sum(vals_modelatt >= 0) / len(vals_modelatt)
            n_neg_per = np.sum(vals_modelatt < 0) / len(vals_modelatt)
            elements_modelatt = [model, att, np.round(summation, 3), np.round(mean, 3),
                                 np.round(std, 3), np.round(CV, 3),
                                 np.round(min, 3), np.round(max, 3),
                                 np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                                 np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
            stats_modelatt.append(elements_modelatt)
            fig = plt.figure(figsize=(10, 8))
            plt.hist(vals_modelatt, bins=20)
            plt.savefig(os.path.join(path_results, 'energyscores',
                                     f'{model}_{att}_energyscores_distribution_mw{min_words}.png'))
            plt.close(fig)

    df = pd.DataFrame(data=stats_label,
                      columns=['model', 'att_module', 'label', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                               'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

    df.to_excel(
        os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_label_mw{min_words}.xlsx'))

    df = pd.DataFrame(data=stats_modelatt,
                      columns=['model', 'att_module', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                               'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

    df.to_excel(
        os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_modelatt_mw{min_words}.xlsx'))


import argparse

parser = argparse.ArgumentParser()
# Model-unspecific commandline arguments
parser.add_argument('-m', '--model',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['CNN', 'BiLSTM', 'BiGRU', 'CLF'],
                    help='Select a predefined model.')

parser.add_argument('-am', '--attention_module',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['random', 'hierarchical_random',
                             'pretrained', 'hierarchical_pretrained'],
                    help='Select a type of predefined attention mechanism or none.')

parser.add_argument('-pre', '--preprocessing',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_CK', '--pre_catcontext_K',
                    default=False,
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_QK', '--pre_querylabel_K',
                    default=False,
                    type=bool,
                    choices=[True, False])

parser.add_argument('-kp', '--process_k_largest_tokens',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-k', '--k_token',
                    type=int,
                    default=3)
parser.add_argument('-mw', '--min_words',
                    type=int,
                    default=3)
parser.add_argument('-kg', '--get_k_largest_tokens',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-l', '--labels',
                    nargs='+',
                    default=['39.95', '36.15', '39.61', '427.31'])
parser.add_argument('-nl', '--n_labels',
                    type=int,
                    default=None)
parser.add_argument('-gp', '--get_phrases',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-ps', '--phrase_size',
                    type=int,
                    default=5)
parser.add_argument('-di', '--doc_indices',
                    type=int,
                    nargs='+',
                    default=None)
parser.add_argument('-cs', '--compute_similarities',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-ges', '--get_energyscore_stats',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gas', '--get_att_scores',
                    type=bool,
                    default=False,
                    choices=[True, False],
                    help='Flag indicating if most important phrases should be retrieved based on the attention scores')
parser.add_argument('-d', '--dataset',
                    type=str,
                    choices=['MimicFull', 'Mimic50'])
parser.add_argument('-ged', '--get_energy_doc',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gedl', '--get_energy_doc_label',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gpf', '--get_phrase_frequencies',
                    type=bool,
                    default=False,
                    choices=[True, False])

args = parser.parse_args()


def main():
    models = args.model
    dataset =args.dataset
    atts = args.attention_module
    pre = args.preprocessing
    pre_CK = args.pre_catcontext_K
    pre_QK = args.pre_querylabel_K
    kp = args.process_k_largest_tokens
    k = args.k_token
    mw = args.min_words
    k_get = args.get_k_largest_tokens
    labels = args.labels
    n_labels = args.n_labels
    get_phrases = args.get_phrases
    ps = args.phrase_size
    doc_ids = args.doc_indices
    ges = args.get_energyscore_stats
    get_att_scores = args.get_att_scores
    get_energy_doc = args.get_energy_doc
    get_energy_doc_label = args.get_energy_doc_label
    get_phrase_frequencies = args.get_phrase_frequencies
    if dataset == 'MimicFull':
        seeds = ['25']
    else:
        seeds = ['13', '25', '42', '87', '111']
    if pre:
        for model in models:
            for att in atts:
                t = time.process_time()
                load_scores(model=model, att=att, dataset=dataset, path_results=path_results, seeds=seeds)

                consolidate_seed_dicts(model=model, att=att, path_results=path_results, dataset=dataset, seeds=seeds)
                elapsed_time = time.process_time() - t
                print(f'Processing time: {elapsed_time}')

    if kp:
        d = {}
        for model in models:
            train_token_freqs = get_train_token_freqs(model=model, )
            d[model] = {}
            for att in atts:
                k_token_dict = get_k_largest_scores(model=model,
                                                    att=att,
                                                    train_token_freqs=train_token_freqs,
                                                    k=k,
                                                    min_words=mw)
                d[model][att] = k_token_dict
        create_dfs_from_dict(d, k=k, min_words=mw)

    if get_energy_doc:

        for doc_id in doc_ids:
            d = {'models': [], 'atts': [], 'rank': [], 'phrase': [], 'mean_energy': [], 'energy': [], 'pos': []}
            for model in models:
                for att in atts:
                    top_k_phrases = load_energy_for_doc(model=model,
                                                        att=att,
                                                        doc_hadm_ids=[doc_id],
                                                        dataset=dataset,
                                                        seed=25)
                    top_k_phrases = get_phrases_for_doc(top_k_phrases=top_k_phrases,
                                                        model=model,
                                                        att=att,
                                                        doc_hadm_ids=[doc_id],
                                                        dataset=dataset,
                                                        seed=25)

                    for rank, phrase in enumerate(top_k_phrases):
                        d['models'].append(model)
                        d['atts'].append(att)
                        d['rank'].append(rank+1)
                        d['phrase'].append(phrase[0])
                        d['mean_energy'].append(phrase[1])
                        d['energy'].append(phrase[2])
                        d['pos'].append(phrase[3])

            df = pd.DataFrame(d)

            df.to_excel(os.path.join(path_results, f'{dataset}', f'phrases_{dataset}_{doc_id}.xlsx'))

    if get_energy_doc_label:
        for model in models:
            for att in atts:
                top_k_phrases = load_energy_for_doc_label(model=model, att=att, dataset=dataset)

    if get_phrase_frequencies:
        for model in models:
            for att in atts:
                get_phrase_frequency(model=model, att=att, dataset=dataset)
    if k_get:
        get_k_tokens(models=models, atts=atts, labels=labels, min_words=mw)

    if get_phrases:
        if doc_ids is None:
            doc_ids = retrieve_docs_with_labels(models=models, atts=atts, labels=labels, n_labels=int(n_labels))
            print(doc_ids)
            if len(doc_ids) == 0:
                quit()

        #    doc_ids = np.random.choice(doc_ids, 1).tolist()
        # print(f'Extract phrases for document with HADM-ID: {doc_ids}')
        for doc_id in doc_ids:
            get_phrases_with_largest_score(models=models,
                                           atts=atts,
                                           labels=labels,
                                           doc_ids=[doc_id],
                                           get_att_scores=get_att_scores)

    if ges:
        compute_energyscores_statistics(models=models,
                                        atts=atts,
                                        hier_cat_context_K=pre_CK,
                                        hier_query_label_K=pre_QK,
                                        min_words=mw)


if __name__ == '__main__':
    main()
