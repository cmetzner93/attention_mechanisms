"""
This file contains source code to process and analyse the energy scores generated by the attention mechanisms during
the testing of the model.
    @author: Christoph Metzner
    @email: cmetzner@vols.utk.edu
    @created: 12/09/2022
    @last modified: 03/23/2023
"""

# built-in libraries
import os
import sys
from typing import List, Dict
import pickle
import h5py
import time
import re

# installed libraries
import numpy as np
import pandas as pd
pd.set_option('display.max_rows', None)
import matplotlib.pyplot as plt

from transformers import AutoTokenizer
CLF_tokenizer = AutoTokenizer.from_pretrained(
    "/Users/cmetzner/Desktop/Study/PhD/research/ORNL/Biostatistics and Multiscale System Modeling/attention_mechanisms/src/models/Clinical-Longformer",
    model_max_length=4096)

# Set up root folder
try:
    root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
except NameError:
    root = os.path.dirname(os.getcwd())
sys.path.append(root)

# Path to storage location of results
path_results = os.path.join('/Volumes', 'MetznerSSD', 'Study_Attention')
# Path to storage location of processed MIMIC-III-50 data
path_proc = os.path.join(root, 'data', 'processed')


def get_train_token_freqs(model: str, path_dataset: str):
    """
    Function that computes or loads a dictionary containing the frequencies for each token in the training corpus.

    Parameters
    ----------
    model : str

    path_dataset : str
        Path to storage location of the sequences of the training corpus

    Returns
    -------
    Dict[int, int]

    """
    from collections import Counter

    if model == 'CLF':
        file_path = os.path.join(path_dataset, 'train_tokens_freqs_trans.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train_text.pkl'))

            train_token_freqs = Counter(element for doc in list(df_train['input_ids'].tolist()) for element in doc)
            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    else:
        file_path = os.path.join(path_dataset, 'train_tokens_freqs.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_train.pkl'))
            train_token_freqs = Counter(element for doc in list(df_train) for element in doc)

            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    return train_token_freqs


def truncate_seq(seq: List[int], max_doc_len: int = 3000):
    """
    Function that either truncates the clinical document sequence (seq) to the maximum length if longer or
    returns its actual length

    Parameters
    ----------
    seq : List[int]
        Unprocessed document sequence
    max_length : int; default=3000
        Maximum document length

    Returns
    -------
    List[int]

    """

    length = len(seq)

    if length > max_doc_len:
        return seq[:max_doc_len]
    return seq[:length]


def load_scores(model: str,
                att: str,
                dataset: str,
                path_results: str,
                seeds=None,
                max_doc_len: int = 3_000) -> None:
    """
    This function retrieves the energy scores per batch of document, contained in .hdf5 files, and matches them
    with the correct token across multiple seeds to take an average energy score value. Finally, a dictionary for each
    seed is created that contains descriptive statistics for each token w.r.t a label in the label space.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    dataset : str
        Name of dataset, e.g., | Mimic50 | MimicFull |
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    seeds : List[str]; default = None
        Seeds used during the experiments
    max_doc_len : int; default=3_000
        Set maximal document length to 3,000 tokens - this is different for the CLF models

    """

    if seeds is None:
        seeds = ['13', '25', '42', '87', '111']

    # Create directory to store processed energy scores
    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed/')
    if not os.path.exists(os.path.dirname(path_res_scores)):
        try:
            print(f'Creating directory at: {path_res_scores}')
            os.makedirs(os.path.dirname(path_res_scores))
        except OSError as error:
            print(error)

    print(f'Retrieving and consolidating energy scores for {model} and {att} label attention.')
    # The text-encoder architectures utilize a different vocabulary / tokenizer; thus load respective object
    if model == 'CLF':
        with open(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'truncated_seq': docs_tensor}
        docs = pd.DataFrame(data=d)
        max_doc_len = 4096
        bs = 4
    else:
        docs = pd.DataFrame(pd.read_pickle(os.path.join(path_proc, f'data_{dataset}', f'X_{dataset}_test.pkl')))
        docs['truncated_seq'] = docs.apply(lambda x: truncate_seq(x.token2id, max_doc_len=max_doc_len), axis=1)
        bs = 16

    # Retrieve labels for dataset
    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept.pkl'), 'rb') as f:
            labels = pickle.load(f)
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept_index.pkl'), 'rb') as f:
            labels_kept_idx = pickle.load(f)
    elif dataset == 'Mimic50':
        with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
            labels = pickle.load(f)

    # Loop throught the different seeds
    for seed in seeds:
        print(f'Current seed: {seed}')

        # Init empty dictionary for current seed
        d_tmp = {}

        # init counter variable for b indicating current batch
        b = 0

        # Use while loop to loop through files since models may have different number of batches
        # depending on batch size
        while True:
            t2 = time.process_time()
            if (b + 1) % 5 == 0:
                print(f'Current batch: {b + 1}')

            # Load hdf5 file containing energy score matrix for current batch
            file_name = f'{model}_{att}_{seed}_test_en_scores_batch{b}.hdf5'
            if dataset == 'MimicFull':
                file = os.path.join(path_results, f'{dataset}', f'{model}', file_name)
            else:
                file = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'analysis', file_name)
            if os.path.isfile(file):
                with h5py.File(file, 'r') as f:
                    scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]
                    if dataset == 'MimicFull':
                        scores = scores[: , labels_kept_idx, :]

                # retrieve sequences for current batch
                docs_batch = docs.iloc[bs * b: (b + 1) * bs]  # retrieve sequences from current batch

                # This loop aligns the energy scores to their tokenid and finally appends the score to a token-specific
                # array. This array is then used to compute descriptive statistics for a token. Appending the individual
                # scores to an array avoids averaging errors.
                for doc in range(docs_batch.shape[0]):
                    for l, label in enumerate(labels):
                        if label not in d_tmp.keys():
                            d_tmp[label] = {}
                        for t, token in enumerate(docs_batch.iloc[doc].truncated_seq):
                            if token not in d_tmp[label].keys():
                                d_tmp[label][token] = []
                            # Retrieve the label-specific (l) energy score of token t in document doc
                            d_tmp[label][token].append(scores[doc][l][t])

                b += 1
                elapsed_time2 = time.process_time() - t2
                print(f'Processing time: {elapsed_time2}')
            else:
                break

        # Take the arrays and compute descriptive statistics
        d_cons = {}
        for l, label in enumerate(labels):
            if label not in d_cons.keys():
                d_cons[label] = {}
            for t, token in enumerate(d_tmp[label].keys()):
                arr = d_tmp[label][token]
                sum_val = np.sum(arr)
                mean = np.mean(arr)
                median = np.median(arr)
                std = np.std(arr)
                min_val = np.min(arr)
                max_val = np.max(arr)
                n = len(arr)
                d_cons[label][token] = {'sum': sum_val,
                                        'mean': mean,
                                        'median': median,
                                        'std': std,
                                        'min': min_val,
                                        'max': max_val,
                                        'n': n}

        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_test_{seed}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_cons, f)


def consolidate_seed_dicts(model: str,
                            att: str,
                            dataset: str,
                            path_results: str,
                            seeds=None):
    """
    This function retrieves the generated seed-specific dictionaries and consolidates the contained token-speicific
    scores w.r.t its label into one general dictionary.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    dataset : str
        Name of dataset, e.g., | Mimic50 | MimicFull |
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    seeds : List[str]; default = None
        Seeds used during the experiments

    """

    if seeds is None:
        seeds = ['13', '25', '42', '87', '111']

    # Set path to retrieved scores
    path_res_scores = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed')

    # Retrieve labels for dataset
    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept.pkl'), 'rb') as f:
            labels = pickle.load(f)
    elif dataset == 'Mimic50':
        with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
            labels = pickle.load(f)

    # Create a new dictionary to hold the consolidated energy scores
    dicts = []
    for seed in seeds:
        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_{seed}_energyscores.pkl'), 'rb') as f:
            d = pickle.load(f)
        dicts.append(d)

    # This code aligns the energy scores for each token per label across the different seeds
    d_total = {}
    for label in labels:
        tokens = dicts[0][label].keys()
        d_total[label] = {}
        for token in tokens:
            sum_arr = []
            mean_arr = []
            median_arr = []
            std_arr = []
            min_arr = []
            max_arr = []

            # Loop through the dictionary containing the energy scores for each seed
            for d_seed in dicts:
                token_dict = d_seed[label][token]
                sum_arr.append(token_dict['sum'])
                mean_arr.append(token_dict['mean'])
                median_arr.append(token_dict['median'])
                std_arr.append(token_dict['std'])
                min_arr.append(token_dict['min'])
                max_arr.append(token_dict['max'])

            d_total[label][token] = {'sum': np.mean(sum_arr),
                                     'mean': np.mean(mean_arr),
                                     'median': np.mean(median_arr),
                                     'std': np.mean(std_arr),
                                     'min': np.mean(min_arr),
                                     'max': np.mean(max_arr),
                                     'n': d_seed[label][token]['n']}

        with open(os.path.join(path_res_scores,
                               f'{model}_{att}_test_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)


def load_energy_for_doc_label(model: str, att: str, dataset: str) -> None:
    """
    Function that retrieves the top 5 phrases with the highest average energy scores
    Parameters
    ----------
    model : str
        Type of model, e.g., CNN, BiGRU, BiLSTM, CLF
    att : str
        Type of attention mechanism, e.g., random, pretrained, hierarchical_random, hierarchical_pretrained
    dataset : str
        Name of dataset, e.g., | Mimic50 | MimicFull |

    """
    # Retrieve labels for dataset
    if dataset == 'MimicFull':
        with open(os.path.join(root, 'results', 'results_section_A', 'labels_kept.pkl'), 'rb') as f:
            labels = pickle.load(f)
    elif dataset == 'Mimic50':
        with open(os.path.join(path_proc, f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
            labels = pickle.load(f)

    # Load vocabulary for dataset
    with open(os.path.join(path_proc, f'data_{dataset}', f'vocab_{dataset}.pkl'), 'rb') as f:
        vocab = pickle.load(f)

    # Load dictionary with energy scores consolidated on label-level
    with open(os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'energy_processed',
                           f'{model}_{att}_test_energyscores.pkl'), 'rb') as f:
        dict_scores = pickle.load(f)

    # Load HADM-IDs associated with documents in test corpus
    doc_ids = pd.read_csv(os.path.join(path_proc, f'data_{dataset}',
                                       f'ids_{dataset}_test.csv')).HADM_ID.tolist()

    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        # Create a pandas dataframe
        d = {'hadm_id': doc_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        max_length = 4096
        phrase_sizes = [4, 6, 8]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = doc_ids
        max_length = 3000
        phrase_sizes = [3, 4, 5]

    # Loop through each label in the label space
    k = 15
    for l, label in enumerate(labels):
        print(f'Label: {label}')
        scores_label = dict_scores[label]
        file_name_results = f'{dataset}_{model}_{att}_top_k_phrases_{label}.csv'

        if not os.path.exists(os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction')):
            os.makedirs(os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction'))

        file_path = os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction', file_name_results)

        # Loop through all documents in the test dataset
        for doc_id in doc_ids:
            # Retrieve integer-index document
            doc = test_docs[test_docs['hadm_id'] == doc_id]['token2id'].item()

            # Remove <pad> token and/or clip to max length
            if model == 'CLF':
                if len(doc) > max_length:
                    doc = doc[:max_length]
                else:
                    phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(doc)
                    doc_no_pad = [token for token in phrase_tokens if token != '<pad>']
                    len_doc = len(doc_no_pad)
                    doc = doc[:len_doc]
            else:
                if len(doc) > max_length:
                    doc = doc[:max_length]

            doc_len = len(doc)

            # Map aggregated energy scores to each integer-index token
            # This retrieves the energy score sequences
            doc_energy = [scores_label[idx]['mean'] for idx in doc]

            # This loops through the sequences with different phrase sizes and extracts the top 5 phrases per document
            means = []
            means_energy = []
            means_idx = []
            for ps in phrase_sizes:
                means_ps = []
                for idx in range(doc_len):
                    phrase_energy = doc_energy[idx:idx + ps]
                    means_ps.append(np.mean(phrase_energy))
                # Retrieve idx for the three phrases with the highest average score
                top_k_idx = list(np.argsort(np.array(means_ps))[::-1][:k])
                for idx in top_k_idx:
                    means.append(means_ps[idx])
                    means_energy.append(doc_energy[idx:idx + ps])
                    means_idx.append(idx)

            # Get top 3 means across the phrase sizes for each label
            top_k_idx = list(np.argsort(np.array(means))[::-1][:k])

            # Add means to array that contains the means across the 3 phrase sizes
            for idx in top_k_idx:
                means.append(means[idx])
                means_energy.append(means_energy[idx])
                means_idx.append(means_idx[idx])

            # Remove duplicate phrases
            means_idx_no_duplicates = []
            means_no_duplicates = []
            means_energy_no_duplicates = []
            for i, idx in enumerate(means_idx):
                if idx not in means_idx_no_duplicates:
                    means_idx_no_duplicates.append(idx)
                    means_no_duplicates.append(means[i])
                    means_energy_no_duplicates.append(means_energy[i])

            # Get top 3 means across the phrase sizes for each label
            k = 5
            top_k_ids = list(np.argsort(np.array(means_no_duplicates))[::-1][:k])
            means = []
            means_energy = []
            means_idx = []
            for idx in top_k_ids:
                means.append(means_no_duplicates[idx])
                means_energy.append(means_energy_no_duplicates[idx])
                means_idx.append(means_idx_no_duplicates[idx])

            while len(means) < 5:
                means.append(0)
                means_energy.append([0])
                means_idx.append(0)

            top_k_phrases = []
            for mean, mean_energy, mean_idx in zip(means, means_energy, means_idx):
                top_k_phrases.append((mean, mean_energy, mean_idx))

            # Init helper dictionary that is casted to pandas csv
            d_helper = {'model': [model] * k, 'att': [att] * k, 'doc_id': [doc_id] * k, 'rank': np.arange(1, k+1),
                        'phrase': [], 'phrase_tokens': [], 'energy_mean': [], 'energy_token': [], 'phrase_pos_idx': []}
            for top_phrase in top_k_phrases:
                mean_score = top_phrase[0]
                phrase_scores = top_phrase[1]
                start_pos = top_phrase[2]

                phrase_idx = doc[start_pos: start_pos + len(phrase_scores)]
                # Decode sequence
                if model == 'CLF':
                    #  phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
                    phrase_tokens = CLF_tokenizer.decode(phrase_idx)
                else:
                    phrase_tokens = vocab.lookup_tokens(phrase_idx)

                d_helper['phrase'].append(" ".join(phrase_tokens))
                d_helper['phrase_tokens'].append(phrase_tokens)
                d_helper['energy_mean'].append(mean_score)
                d_helper['energy_token'].append(phrase_scores)
                d_helper['phrase_pos_idx'].append(start_pos)

            df = pd.DataFrame(d_helper)
            if os.path.isfile(file_path):
                df.to_csv(file_path, mode='a', header=False)
            else:
                df.to_csv(file_path, header=d_helper.keys())

        df_label = pd.read_csv(file_path).reset_index(drop=True)
        df_label.to_csv(file_path)


def get_phrase_frequency(model: str, att: str, dataset: str) -> None:
    """
    This function retrieves the number of phrases
    Parameters
    ----------
    model : str
        Type of model, e.g., CNN, BiGRU, BiLSTM, CLF
    att : str
        Type of attention mechanism, e.g., random, pretrained, hierarchical_random, hierarchical_pretrained
    dataset : str
        Name of dataset, e.g., | Mimic50 | MimicFull |

    """
    # Load labels for MIMIC-III-50
    with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
        mimic50_labels = pickle.load(f)

    # Load category descriptions
    code_descs = pd.read_pickle(os.path.join(root, 'data', 'processed', 'code_descriptions.pkl'))
    code_descs = code_descs.reset_index(drop=True)
    #code_descs = code_descs.drop(['desc_clean', 'desc_len'], axis=1)
    df_mimic50_desc = code_descs[code_descs['ICD9_CODE'].isin(mimic50_labels)].reset_index(drop=True)
    df_mimic50_desc.set_index('ICD9_CODE', inplace=True)
    mimic50_desc = df_mimic50_desc.to_dict()['LONG_TITLE']

    d = {'model': [], 'att': [], 'ICD9_code': [], 'ICD9_code_desc': [], 'rank': [], 'phrase': [], 'count': []}

    for l, label in enumerate(mimic50_labels):
        print(f'Label: {label}')
        file_name_results = f'{dataset}_{model}_{att}_top_k_phrases_{label}.csv'
        file_path = os.path.join(path_results, f'{dataset}', 'analysis_phrase_extraction', file_name_results)
        df = pd.read_csv(file_path).reset_index(drop=True)
        df = df.drop(['Unnamed: 0'], axis=1)

        phrase_counts = df.phrase.value_counts()[:5]

        for count, phrase in enumerate(phrase_counts.index.tolist()):
            if model == 'CLF':
                # remove spaces after each character
                phrase = ' '.join([re.sub(' ', '', token) for token in phrase.split('  ')]).strip()
            d['ICD9_code'].append(label)
            d['ICD9_code_desc'].append(mimic50_desc[label])
            d['model'].append(model)
            d['att'].append(att)
            d['rank'].append(count+1)
            d['phrase'].append(phrase)
            d['count'].append(phrase_counts[count])

    df = pd.DataFrame(d)

    df.to_csv(os.path.join(path_results, f'{dataset}', f'{dataset}_{model}_{att}_top_k_phrases.csv'),
              header=True, index=True)


def get_phrases_for_doc(top_k_phrases, model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 42):
    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]

    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()

    phrases = []
    for top_phrase in top_k_phrases:
        mean_score = top_phrase[0]
        phrase_scores = top_phrase[1]
        len_seq = phrase_scores.shape[0]
        start_pos = top_phrase[2]

        phrase_idx = seq[start_pos: start_pos + len_seq]
        # Decode sequence
        if model == 'CLF':
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
        else:
            with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'vocab_{dataset}.pkl'), 'rb') as f:
                vocab = pickle.load(f)
            phrase_tokens = vocab.lookup_tokens(phrase_idx)
        phrases.append((phrase_tokens, mean_score, phrase_scores, start_pos))

    print(f'{model}/{att}: {phrases}')
    return phrases


def load_energy_for_doc(model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 25):
    """
    Function that retrieves the energy scores for one specific
    Parameters
    ----------
    model : str
        Type of model, e.g., CNN, BiGRU, BiLSTM, CLF
    att : str
        Type of attention mechanism, e.g., random, pretrained, hierarchical_random, hierarchical_pretrained
    Returns
    -------

    """
    if dataset == 'MimicFull':
        with open(os.path.join(path_results, 'performance_results', 'mimic_full', 'labels_kept_index.pkl'), 'rb') as f:
            labels = pickle.load(f)
    elif dataset == 'Mimic50':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'l_codes_{dataset}.pkl'), 'rb') as f:
            labels = pickle.load(f)


    # Load testids for dataset
    test_ids = pd.read_csv(
        os.path.join(root, 'data', 'processed', f'data_{dataset}', f'ids_{dataset}_test.csv')).HADM_ID.tolist()

    # We want to match the phrases highlighted by a medical resident with the phrases extracted from our models
    # Given that the energy score file size is so large we are not processing the energy scores to get an
    # aggregated value for each token --> we only care about the energy scores for a given document
    # To do that, we need to extract the specific location of each document in the ordered test dataset and
    # therefore, locate them in each of the batches

    # First: Retrieve absolute position of each document in doc_hadm_ids_marked in test dataset
    doc_idx = [test_ids.index(idx) for idx in doc_hadm_ids]

    # Third: Retrieve the batch number and the specific doc number in the batch
    if model == 'CLF':
        batch_size = 4  # batch_size
        max_doc_len = 4096
    else:
        batch_size = 16  # batch_size
        max_doc_len = 3000

    # Energyscores are in same order as test documents during testing
    # Energyscores of each batch are stored separately
    batch_vals = []
    for hadm_id, doc_id in zip(doc_hadm_ids, doc_idx):
        b = doc_id // batch_size
        doc_idx_in_batch = doc_id % batch_size - 1
        if doc_idx_in_batch == -1:
            doc_idx_in_batch = 0
        #print(f'{hadm_id}/{doc_id}: bn(doc_pos): {b}({doc_idx_in_batch})')
        batch_vals.append((b, doc_idx_in_batch))

    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        phrase_sizes = [4, 6, 8]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        phrase_sizes = [3, 4, 5]

    # Check length of sequence
    # Get sequence
    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()
    # Need to remove <pad>
    if model == 'CLF':
        if len(seq) > max_doc_len:
            seq = seq[:max_doc_len]
        else:
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(seq)
            # This removes all padding tokens: <pad>
            seq = [token for token in phrase_tokens if token != '<pad>']
    else:
        if len(seq) > max_doc_len:
            seq = seq[:max_doc_len]
    seq_length = len(seq)
    # Load energy score file
    for b, doc_idx_in_batch in batch_vals:
        file_name = f'{model}_{att}_{seed}_test_en_scores_batch{b}.hdf5'
        if dataset == 'MimicFull':
            file = os.path.join(path_results, f'{dataset}', f'{model}', file_name)
        else:
            file = os.path.join(path_results, f'{dataset}', f'{model}', 'scores', 'analysis', file_name)
        if os.path.isfile(file):
            with h5py.File(file, 'r') as f:
                scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]
                if dataset == 'MimicFull':
                    scores = scores[doc_idx_in_batch, labels, :]
                else:
                    scores = scores[doc_idx_in_batch, :, :]

        # Extract phrases with largest average energy scores for 3, 4, 5 and 4, 6, 8 tokens
        # 1. Loop to extract key phrases for different window sizes across all considered labels

        # Loop through energy score matrix
        k = 10
        means = []
        means_energy = []
        means_idx = []
        for l, label in enumerate(labels):
            scores_label = scores[l]

            label_means = []
            label_means_energy = []
            label_means_idx = []
            # Get subsequences from sequences for 3 (4), 4 (6), and 5 (8) tokens for CNN/RNN (CLF).
            for ps in phrase_sizes:
                means_ps = []
                # Loop through sequence
                for idx in range(seq_length):
                    phrase_energy = scores_label[idx:idx+ps]
                    means_ps.append(np.mean(phrase_energy))



                # Retrieve idx for the three phrases with the highest average score
                top_k_idx = list(np.argsort(np.array(means_ps))[::-1][:k])

                for idx in top_k_idx:
                    label_means.append(means_ps[idx])
                    label_means_energy.append(scores_label[idx:idx+ps])
                    label_means_idx.append(idx)

            # Get top 3 means across the phrase sizes for each label
            top_k_idx = list(np.argsort(np.array(label_means))[::-1][:k])

            # Add means to array that contains the means across the 3 phrase sizes
            for idx in top_k_idx:
                means.append(label_means[idx])
                means_energy.append(label_means_energy[idx])
                means_idx.append(label_means_idx[idx])

        # Remove duplicate phrases
        means_idx_no_duplicates = []
        means_no_duplicates = []
        means_energy_no_duplicates = []
        for i, idx in enumerate(means_idx):
            if idx not in means_idx_no_duplicates:
                means_idx_no_duplicates.append(idx)
                means_no_duplicates.append(means[i])
                means_energy_no_duplicates.append(means_energy[i])

        # Loop to remove direct neighbors
        means_idx_old = means_idx_no_duplicates.copy()
        for i, idi in enumerate(means_idx_no_duplicates):
            ub = idi + 2
            lb = idi - 2
            for j, idj in enumerate(means_idx_no_duplicates[:]):
                if i != j:
                    if ub >= idj >= lb:
                        means_idx_no_duplicates.remove(idj)  # this requires the duplicate removal process

        means_no_duplicates = [means_no_duplicates[means_idx_old.index(idx)] for idx in means_idx_no_duplicates]
        means_energy_no_duplicates = [means_energy_no_duplicates[means_idx_old.index(idx)] for idx in means_idx_no_duplicates]

        # Get top 3 means across the phrase sizes for each label
        top_k_ids = list(np.argsort(np.array(means_no_duplicates))[::-1][:k])
        means = []
        means_energy = []
        means_idx = []
        for idx in top_k_ids:
            means.append(means_no_duplicates[idx])
            means_energy.append(means_energy_no_duplicates[idx])
            means_idx.append(means_idx_no_duplicates[idx])

        top_k_phrases = []
        for mean, mean_energy, mean_idx in zip(means, means_energy, means_idx):
            top_k_phrases.append((mean, mean_energy, mean_idx))

        return top_k_phrases


def get_phrases_for_doc(top_k_phrases, model: str, att: str, doc_hadm_ids: int, dataset: str, seed: int = 42):
    test_ids = pd.read_csv(os.path.join(root, 'data', 'processed', f'data_{dataset}',
                                        f'ids_{dataset}_test.csv')).HADM_ID.tolist()
    if model == 'CLF':
        with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'hadm_id': test_ids, 'token2id': docs_tensor}
        test_docs = pd.DataFrame(data=d)
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 4096
        #phrase_sizes = [6, 8, 10]

    else:  # CNN and BiGRU / BiLSTM data
        test_docs = pd.DataFrame(
            pd.read_pickle(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'X_{dataset}_test.pkl')))
        test_docs['hadm_id'] = test_ids
        test_docs = test_docs[test_docs['hadm_id'].isin(doc_hadm_ids)]
        #doc_length = 3000
        #test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
        #phrase_sizes = [3, 4, 5]

    seq = test_docs[test_docs['hadm_id'] == doc_hadm_ids[0]]['token2id'].item()

    phrases = []
    for top_phrase in top_k_phrases:
        mean_score = top_phrase[0]
        phrase_scores = top_phrase[1]
        len_seq = phrase_scores.shape[0]
        start_pos = top_phrase[2]

        phrase_idx = seq[start_pos: start_pos + len_seq]
        # Decode sequence
        if model == 'CLF':
            phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
        else:
            with open(os.path.join(root, 'data', 'processed', f'data_{dataset}', f'vocab_{dataset}.pkl'), 'rb') as f:
                vocab = pickle.load(f)
            phrase_tokens = vocab.lookup_tokens(phrase_idx)
        phrases.append((phrase_tokens, mean_score, phrase_scores, start_pos))

    print(f'{model}/{att}: {phrases}')
    return phrases


def compute_energyscores_statistics(models: str,
                                    atts: str,
                                    min_words: int = 3):
    stats_label = []
    stats_modelatt = []
    for model in models:
        path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

        for att in atts:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_test_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)
            vals_modelatt = []
            for label in d_scores.keys():
                vals = []
                for token in d_scores[label].keys():
                    if d_scores[label][token]['n'] >= min_words:
                        vals.append(d_scores[label][token]['mean'])
                        vals_modelatt.append(d_scores[label][token]['mean'])
                vals = np.array(vals, dtype=np.float64)
                summation = np.sum(np.array(list(map(abs, vals)), dtype=np.float64))
                mean = np.mean(vals)
                std = np.std(vals)
                min = np.min(vals)
                max = np.max(vals)
                CV = std / mean
                median = np.median(vals)
                Q25 = np.quantile(vals, 0.25)
                Q75 = np.quantile(vals, 0.75)
                n_pos_per = np.sum(vals >= 0) / len(vals)
                n_neg_per = np.sum(vals < 0) / len(vals)
                elements = [model, att, label,
                            np.round(summation, 3), np.round(mean, 3), np.round(std, 3),
                            np.round(CV, 3), np.round(min, 3), np.round(max, 3),
                            np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                            np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
                stats_label.append(elements)

            vals_modelatt = np.array(vals_modelatt, dtype=np.float64)
            summation = np.sum(np.array(list(map(abs, vals_modelatt)), dtype=np.float64))
            mean = np.mean(vals_modelatt)
            std = np.std(vals_modelatt)
            min = np.min(vals_modelatt)
            max = np.max(vals_modelatt)
            CV = abs(std) / abs(mean)
            median = np.median(vals_modelatt)
            Q25 = np.quantile(vals_modelatt, 0.25)
            Q75 = np.quantile(vals_modelatt, 0.75)
            n_pos_per = np.sum(vals_modelatt >= 0) / len(vals_modelatt)
            n_neg_per = np.sum(vals_modelatt < 0) / len(vals_modelatt)
            elements_modelatt = [model, att, np.round(summation, 3), np.round(mean, 3),
                                 np.round(std, 3), np.round(CV, 3),
                                 np.round(min, 3), np.round(max, 3),
                                 np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                                 np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
            stats_modelatt.append(elements_modelatt)
            fig = plt.figure(figsize=(10, 8))
            plt.hist(vals_modelatt, bins=20)
            plt.savefig(os.path.join(path_results, 'energyscores',
                                     f'{model}_{att}_energyscores_distribution_mw{min_words}.png'))
            plt.close(fig)

    df = pd.DataFrame(data=stats_label,
                      columns=['model', 'att_module', 'label', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                               'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

    df.to_excel(
        os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_label_mw{min_words}.xlsx'))

    df = pd.DataFrame(data=stats_modelatt,
                      columns=['model', 'att_module', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                               'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

    df.to_excel(
        os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_modelatt_mw{min_words}.xlsx'))


import argparse

parser = argparse.ArgumentParser()
# Model-unspecific commandline arguments
parser.add_argument('-m', '--model',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['CNN', 'BiLSTM', 'BiGRU', 'CLF'],
                    help='Select a predefined model.')
parser.add_argument('-d', '--dataset',
                    type=str,
                    choices=['MimicFull', 'Mimic50'])
parser.add_argument('-am', '--attention_module',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['random', 'hierarchical_random',
                             'pretrained', 'hierarchical_pretrained'],
                    help='Select a type of predefined attention mechanism or none.')
parser.add_argument('-pre', '--preprocessing',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_CK', '--pre_catcontext_K',
                    default=False,
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_QK', '--pre_querylabel_K',
                    default=False,
                    type=bool,
                    choices=[True, False])
parser.add_argument('-mw', '--min_words',
                    type=int,
                    default=3)
parser.add_argument('-di', '--doc_indices',
                    type=int,
                    nargs='+',
                    default=None)
parser.add_argument('-ges', '--get_energyscore_stats',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-ged', '--get_energy_doc',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gedl', '--get_energy_doc_label',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gpf', '--get_phrase_frequencies',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gpfc', '--get_phrase_frequencies_consolidated',
                    type=bool,
                    default=False,
                    choices=[True, False])
args = parser.parse_args()


def main():
    models = args.model
    dataset = args.dataset
    atts = args.attention_module
    pre = args.preprocessing
    pre_CK = args.pre_catcontext_K
    pre_QK = args.pre_querylabel_K
    mw = args.min_words
    doc_ids = args.doc_indices
    ges = args.get_energyscore_stats
    get_energy_doc = args.get_energy_doc
    get_energy_doc_label = args.get_energy_doc_label
    get_phrase_frequencies = args.get_phrase_frequencies
    get_phrase_frequencies_consolidated = args.get_phrase_frequencies_consolidated
    if dataset == 'MimicFull':
        seeds = ['25']
    else:
        seeds = ['13', '25', '42', '87', '111']
    if pre:
        for model in models:
            for att in atts:
                t = time.process_time()
                load_scores(model=model, att=att, dataset=dataset, path_results=path_results, seeds=seeds)

                consolidate_seed_dicts(model=model, att=att, path_results=path_results, dataset=dataset, seeds=seeds)
                elapsed_time = time.process_time() - t
                print(f'Processing time: {elapsed_time}')

    if get_energy_doc:

        for doc_id in doc_ids:
            d = {'models': [], 'atts': [], 'rank': [], 'phrase': [], 'mean_energy': [], 'energy': [], 'pos': []}
            for model in models:
                for att in atts:
                    top_k_phrases = load_energy_for_doc(model=model,
                                                        att=att,
                                                        doc_hadm_ids=[doc_id],
                                                        dataset=dataset,
                                                        seed=25)
                    top_k_phrases = get_phrases_for_doc(top_k_phrases=top_k_phrases,
                                                        model=model,
                                                        att=att,
                                                        doc_hadm_ids=[doc_id],
                                                        dataset=dataset,
                                                        seed=25)

                    for rank, phrase in enumerate(top_k_phrases):
                        d['models'].append(model)
                        d['atts'].append(att)
                        d['rank'].append(rank+1)
                        d['phrase'].append(phrase[0])
                        d['mean_energy'].append(phrase[1])
                        d['energy'].append(phrase[2])
                        d['pos'].append(phrase[3])

            df = pd.DataFrame(d)

            df.to_excel(os.path.join(path_results, f'{dataset}', f'phrases_{dataset}_{doc_id}.xlsx'))

    if get_energy_doc_label:
        for model in models:
            for att in atts:
                top_k_phrases = load_energy_for_doc_label(model=model, att=att, dataset=dataset)

    if get_phrase_frequencies:
        for model in models:
            for att in atts:
                get_phrase_frequency(model=model, att=att, dataset=dataset)

    if get_phrase_frequencies_consolidated:
        file_name = f'{dataset}_top_k_phrases.csv'
        file_path = os.path.join(path_results, f'{dataset}', file_name)
        for model in models:
            for att in atts:
                df = pd.read_csv(os.path.join(path_results, f'{dataset}', f'{dataset}_{model}_{att}_top_k_phrases.csv'), index_col=0)
                if os.path.isfile(file_path):
                    df.to_csv(file_path, mode='a', header=False)
                else:
                    df.to_csv(file_path, header=df.keys())
        df = pd.read_csv(file_path)
        df = df.drop(['Unnamed: 0'], axis=1)
        df.to_csv(file_path)

    if ges:
        compute_energyscores_statistics(models=models,
                                        atts=atts,
                                        hier_cat_context_K=pre_CK,
                                        hier_query_label_K=pre_QK,
                                        min_words=mw)


if __name__ == '__main__':
    main()
