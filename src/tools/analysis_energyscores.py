"""
This file contains source code to process and analyse the energy scores generated by the attention mechanisms during
the testing of the model.
    @author: Christoph Metzner
    @email: cmetzner@vols.utk.edu
    @created: 12/09/2022
    @last modified: 12/09/2022
"""


# built-in libraries
import os
import sys
from typing import List, Dict, Union
import pickle
import h5py
import time

# installed libraries
import numpy as np
np.random.seed(42)
import pandas as pd
pd.set_option('display.max_rows', None)

import matplotlib.pyplot as plt

from transformers import AutoTokenizer

CLF_tokenizer = AutoTokenizer.from_pretrained("/Users/cmetzner/Desktop/Study/PhD/research/ORNL/Biostatistics and Multiscale System Modeling/attention_mechanisms/src/models/Clinical-Longformer", model_max_length=4096)

# Set up root folder
try:
    root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
except NameError:
    root = os.path.dirname(os.getcwd())
sys.path.append(root)
print('Project root: {}'.format(root))

# Setup paths here

# Path to storage location of processed MIMIC-III-50 data
path_mimic50 = os.path.join(root, 'data', 'processed', 'data_Mimic50')
path_hadm50 = os.path.join(root, 'data', 'processed', 'hadm_ids')

# Path to storage location of results
path_results = os.path.join('/Volumes', 'MetznerSSD', 'paper_attention')

# Vocabulary based on training documents - torchtext vocab object - https://pytorch.org/text/stable/vocab.html
with open(os.path.join(path_mimic50, 'vocab_Mimic50.pkl'), 'rb') as f:
    mimic50_vocab = pickle.load(f)

# Load medical code labels
with open(os.path.join(path_mimic50, 'l_codes_Mimic50.pkl'), 'rb') as f:
    mimic50_labels = pickle.load(f)

# Load category labels
with open(os.path.join(path_mimic50, 'l_cats_Mimic50.pkl'), 'rb') as f:
    mimic50_cats = pickle.load(f)

with open(os.path.join(path_mimic50, 'l_codes_Mimic50_features.pkl'), 'rb') as f:
    mimic50_features = pickle.load(f)

# Load category descriptions
mimic50_desc = pd.read_pickle(os.path.join(path_mimic50, 'Mimic50_descriptions.pkl'))

mimic50_desc_dict = dict(zip(mimic50_desc.label, mimic50_desc.description))

# Needed to create lists that contain the column headers in excel sheet containing the overall, quartile,
# and individual results
cols_ind = [f'f1_micro_{label}' for label in mimic50_labels]
cols_q = ["f1_macro_Q0", "f1_micro_Q0", "f1_macro_Q1", "f1_micro_Q1", "f1_macro_Q2", "f1_micro_Q2", "f1_macro_Q3",
          "f1_micro_Q3"]
cols_overall = ['f1_macro_sk', 'f1_micro_sk', 'auc_micro', 'auc_macro', 'prec@5', 'prec@8', 'prec@15']


def get_train_token_freqs(model, path_dataset: str):
    """
    Function that computes or loads a dictionary containing the frequencies for each token in the training corpus.

    Parameters
    ----------
    path_dataset : str
        Path to storage location of the sequences of the training corpus

    Returns
    -------
    Dict[int, int]

    """
    from collections import Counter

    if model == 'CLF':
        file_path = os.path.join(path_dataset, 'train_tokens_freqs_trans.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_mimic50, f'X_Mimic50_train_text.pkl'))

            train_token_freqs = Counter(element for doc in list(df_train['input_ids'].tolist()) for element in doc)
            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    else:
        file_path = os.path.join(path_dataset, 'train_tokens_freqs.pkl')
        if os.path.isfile(file_path):
            with open(file_path, 'rb') as f:
                train_token_freqs = pickle.load(f)
        else:
            # Load training documents to extract word frequency
            df_train = pd.read_pickle(os.path.join(path_mimic50, f'X_Mimic50_train.pkl'))
            train_token_freqs = Counter(element for doc in list(df_train) for element in doc)

            with open(file_path, 'wb') as f:
                pickle.dump(train_token_freqs, f)
    return train_token_freqs


def truncate_seq(seq: List[int], max_doc_len: int = 3000):
    """
    Function that either truncates the clinical document sequence (seq) to the maximum length if longer or
    returns its actual length

    Parameters
    ----------
    seq : List[int]
        Unprocessed document sequence
    max_length : int; default=3000
        Maximum document length

    Returns
    -------
    List[int]

    """

    length = len(seq)

    if length > max_doc_len:
        return seq[:max_doc_len]
    return seq[:length]


def load_scores(model: str,
                att: str,
                path_dataset: str,
                path_results: str,
                split: str = 'test',
                seeds: List[str] = ['13', '25', '42', '87', '111'],
                max_doc_len: int = 3_000,
                hier_cat_context_K: bool = False,
                hier_query_label_K: bool = False):
    """
    This function retrieves the energy scores per batch of document, contained in .hdf5 files, and matches them
    with the correct token across multiple seeds to take an average energy score value. Finally, a dictionary for each
    seed is created that contains descriptive statistics for each token w.r.t a label in the label space.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_dataset : str
        Path to directory where documents are stored
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments
    hier_cat_context_K : bool; default=False
        Flag indicating to retrieve energy scores between the category context vectors and the input sequence K for the
         hierarchical random and hierarchical pretrained attention mechanisms
    hier_cat_context_K : bool; default=False
        Flag indicating to retrieve energy scores between the initial query label vectors and the input sequence K for
         the hierarchical random and hierarchical pretrained attention mechanisms

    """
    # Create directory to store energy score dictionaries if it does not exist
    path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')
    if not os.path.exists(os.path.dirname(path_res_energy)):
        try:
            print(f'Creating directory at: {path_res_energy}')
            os.makedirs(os.path.dirname(path_res_energy))
        except OSError as error:
            print(error)
    print(f'Currently retrieving and consolidating energy scores for {model} and {att} label attention.')

    if model == 'CLF':
        with open(os.path.join(path_dataset, f'X_Mimic50_test_text.pkl'), 'rb') as f:
            docs_tensor = pickle.load(f)['input_ids'].tolist()
        d = {'truncated_seq': docs_tensor}
        docs = pd.DataFrame(data=d)
        max_doc_len = 4096
        bs = 4
    else:
        docs = pd.DataFrame(pd.read_pickle(os.path.join(path_dataset, f'X_Mimic50_test.pkl')))
        docs['truncated_seq'] = docs.apply(lambda x: truncate_seq(x.token2id, max_doc_len=max_doc_len), axis=1)

        bs = 16

    if hier_cat_context_K:
        labels = mimic50_cats
    else:
        labels = mimic50_labels

    for seed in seeds:
        print(f'Current seed: {seed}')

        # Init empty dictionary for current seed
        d_tmp = {}

        # init counter variable for b indicating current batch
        b = 0

        # Use while loop to loop through files since models may have different number of batches
        # depending on batch size
        while True:
            t2 = time.process_time()
            if (b + 1) % 5 == 0:
                print(f'Current batch: {b + 1}')

            # Load hdf5 file containing energy score matrix for current batch
            # Select type of energy scores that should be retrieved and processed
            if hier_cat_context_K:  # Hierarchical Attention: Category context matrix C1 times K
                file_name = f'{model}_{att}_{seed}_en_scores_EC1K_batch{b}.hdf5'
            elif hier_query_label_K:  # Hierarchical Attention: Unenriched query label matrix Q2 times K
                file_name = f'{model}_{att}_{seed}_en_scores_EQ2K_batch{b}.hdf5'
            else:  # All attention mechanisms
                file_name = f'{model}_{att}_{seed}_{split}_en_scores_batch{b}.hdf5'
            file = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'analysis', file_name)
            if os.path.isfile(file):
                with h5py.File(file, 'r') as f:
                    scores = np.array(f['scores'][()])  # shape [batch size, n_labels, n_tokens]

                # retrieve sequences for current batch
                docs_batch = docs.iloc[bs * b : (b + 1) * bs]  # retrieve sequences from current batch

                # This loop aligns the energy scores to their tokenid and finally appends the score to a token-specific
                # array. This array is then used to compute descriptive statistics for a token. Appending the individual
                # scores to an array avoids averaging errors.
                for doc in range(docs_batch.shape[0]):
                    for l, label in enumerate(labels):
                        if label not in d_tmp.keys():
                            d_tmp[label] = {}
                        for t, token in enumerate(docs_batch.iloc[doc].truncated_seq):
                            if token not in d_tmp[label].keys():
                                d_tmp[label][token] = []
                            # Retrieve the label-specific (l) energy score of token t in document doc
                            d_tmp[label][token].append(scores[doc][l][t])

                b += 1
                elapsed_time2 = time.process_time() - t2
                print(f'Processing time: {elapsed_time2}')
            else:
                break

        # Take the arrays and compute descriptive statistics
        d_cons = {}
        for l, label in enumerate(labels):
            if label not in d_cons.keys():
                d_cons[label] = {}
            for t, token in enumerate(d_tmp[label].keys()):
                arr = d_tmp[label][token]
                sum_val = np.sum(arr)
                mean = np.mean(arr)
                median = np.median(arr)
                std = np.std(arr)
                min_val = np.min(arr)
                max_val = np.max(arr)
                n = len(arr)
                d_cons[label][token] = {'sum': sum_val,
                                                'mean': mean,
                                                'median': median,
                                                'std': std,
                                                'min': min_val,
                                                'max': max_val,
                                                'n': n}

        if hier_cat_context_K:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_EC1K_energyscores.pkl'), 'wb') as f:
                pickle.dump(d_cons, f)
        elif hier_query_label_K:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_EQ2K_energyscores.pkl'), 'wb') as f:
                pickle.dump(d_cons, f)
        else:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_energyscores.pkl'), 'wb') as f:
                pickle.dump(d_cons, f)


def consolidate_seed_dicts(model: str,
                           att: str,
                           path_results: str,
                           split: str = 'test',
                           seeds: List[str] = ['13', '25', '42', '87', '111'],
                           hier_cat_context_K: bool = False,
                           hier_query_label_K: bool = False):
    """
    This function retrieves the generated seed-specific dictionaries and consolidates the contained token-speicific
    scores w.r.t its label into one general dictionary.

    Parameters
    -----------
    model : str
        Type of text encoder architecture, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Type of label attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_random |
    path_results : str
        Path to directory where the raw .hdf5 files are stored
    split : str; default='test'
        Data split
    seeds : List[str]; default=['13', '25', '42', '87', '111']
        Seeds used during the experiments
    hier_cat_context_K : bool; default=False
        Flag indicating to retrieve energy scores between the category context vectors and the input sequence K for the
         hierarchical random and hierarchical pretrained attention mechanisms
    hier_cat_context_K : bool; default=False
        Flag indicating to retrieve energy scores between the initial query label vectors and the input sequence K for
         the hierarchical random and hierarchical pretrained attention mechanisms

    """

    path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed')

    if hier_cat_context_K:
        labels = mimic50_cats
    else:
        labels = mimic50_labels

    dicts = []
    for seed in seeds:
        if hier_cat_context_K:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_EC1K_energyscores.pkl'), 'rb') as f:
                d = pickle.load(f)
        elif hier_query_label_K:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_EQ2K_energyscores.pkl'), 'rb') as f:
                d = pickle.load(f)
        else:
            with open(os.path.join(path_res_energy,
                                   f'{model}_{att}_{split}_{seed}_energyscores.pkl'), 'rb') as f:
                d = pickle.load(f)
        dicts.append(d)

    d_total = {}
    for label in labels:
        tokens = dicts[0][label].keys()
        d_total[label] = {}
        for token in tokens:
            sum_arr = []
            mean_arr = []
            median_arr = []
            std_arr = []
            min_arr = []
            max_arr = []
            for d_seed in dicts:
                token_dict = d_seed[label][token]
                sum_arr.append(token_dict['sum'])
                mean_arr.append(token_dict['mean'])
                median_arr.append(token_dict['median'])
                std_arr.append(token_dict['std'])
                min_arr.append(token_dict['min'])
                max_arr.append(token_dict['max'])


            d_total[label][token] = {'sum': np.mean(sum_arr),
                             'mean': np.mean(mean_arr),
                             'median': np.mean(median_arr),
                             'std': np.mean(std_arr),
                             'min': np.mean(min_arr),
                             'max': np.mean(max_arr),
                             'n': d_seed[label][token]['n']}

    if hier_cat_context_K:
        with open(os.path.join(path_res_energy,
                               f'{model}_{att}_{split}_EC1K_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)
    elif hier_query_label_K:
        with open(os.path.join(path_res_energy,
                               f'{model}_{att}_{split}_EQ2K_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)
    else:
        with open(os.path.join(path_res_energy,
                               f'{model}_{att}_{split}_energyscores.pkl'), 'wb') as f:
            pickle.dump(d_total, f)


def get_token_ids(model: str, att: str, split: str) -> List[int]:
    """
    Helper function that extract the list of token indices contained in the energy score dictionaries

    Parameters
    ----------
    model : str
        Model name, e.g., | CNN | BiGRU | BiLSTM | CLF |
    att : str
        Name of attention mechanisms, e.g., | random | pretrained | hierarchical_random | hierarchical_pretrained |
    split : str
        Dataset split

    Returns
    -------
    List[int]
        List containing the token indices

    """

    if model == 'CLF':
        if os.path.isfile(os.path.join(path_results, f'token_ids_{split}_text.pkl')):
            with open(os.path.join(path_results, f'token_ids_{split}_text.pkl'), 'rb') as f:
                token_ids = pickle.load(f)
        else:
            with open(os.path.join(path_results, f'results_analysis_{model}', 'scores',
                                   f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)
            token_ids = list(d_scores[list(d_scores.keys())[0]].keys())
            with open(os.path.join(path_results, f'token_ids_{split}_text.pkl'), 'wb') as f:
                pickle.dump(token_ids, f)
    else:
        if os.path.isfile(os.path.join(path_results, f'token_ids_{split}.pkl')):
            with open(os.path.join(path_results, f'token_ids_{split}.pkl'), 'rb') as f:
                token_ids = pickle.load(f)
        else:
            with open(os.path.join(path_results, f'results_analysis_{model}', 'scores',
                                   f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                d_scores = pickle.load(f)

            token_ids = list(d_scores[list(d_scores.keys())[0]].keys())
            with open(os.path.join(path_results, f'token_ids_{split}.pkl'), 'wb') as f:
                pickle.dump(token_ids, f)

    return token_ids


def get_k_largest_scores(model: List[str],
                         att: List[str],
                         train_token_freqs: Dict[int, int],
                         split: str = 'test',
                         min_words: int = 3,
                         k: int = 3,
                         stat: str = 'mean'):
    """
    This function matches the energy scores to its tokens in a document for multiple seeds

    """
    path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

    # load list of tokens in dataset
    token_ids = get_token_ids(model=model, att=att, split=split)

    scores = {}
    # Load the created dictionary
    with open(os.path.join(path_res_energy,
                           f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
        dict_scores = pickle.load(f)

    for l, label in enumerate(dict_scores):
        stat_tmp = []
        tokens = []
        tokens_tmp = []

        for token in token_ids:
            if dict_scores[label][token]['n'] > min_words:
                stat_tmp.append(dict_scores[label][token][stat])
                tokens.append(token)
                tokens_tmp.append(dict_scores[label][token])

        stat_tmp_arr = np.array(stat_tmp)
        tokens_arr = np.array(tokens)
        tokens_tmp_arr = np.array(tokens_tmp)

        idx_stat_max = np.argsort(stat_tmp_arr)[:-k - 1:-1]
        tokens_arr_max = tokens_arr[idx_stat_max]
        tokens_tmp_arr_max = tokens_tmp_arr[idx_stat_max]

        # add vocab lookup from vocab to add word
        # token2id and id2token
        for token_id, token_d in zip(tokens_arr_max, tokens_tmp_arr_max):
            if model == 'CLF':
                token = CLF_tokenizer.convert_ids_to_tokens([token_id])[0]
            else:
                token = mimic50_vocab.lookup_token(token_id)
            token_d['token'] = token
            token_d['id'] = token_id
            token_d['train_n'] = train_token_freqs[token_id]

        # top_words = [dict_scores[label][token] for token in tokens_arr_max]
        scores[label] = tokens_tmp_arr_max

    return scores


def create_dfs_from_dict(d: Dict[str, float], k: int, min_words: int):
    # Create multiindex and multiindex columns for dataframe
    token_features = ['token', 'mean', 'std', 'n', 'train_n']  # features in lower col index
    lower_col_index = token_features * k  # have to multiple features for k tokens
    upper_col_index = []
    for i in range(k):
        tokens = [f'token_{i + 1}'] * len(token_features)
        upper_col_index += tokens
    col_index = [upper_col_index, lower_col_index]
    row_index = [list(mimic50_desc.label),
                 list(mimic50_desc.description),
                 list(mimic50_desc.frequency),
                 list(mimic50_desc.quartile)]

    # Create one nested dictionary that contains the tokens with the highest energy scores
    dfs = {}

    # Loop through all the considered models (i.e., text encoder architectures)
    for model in d:
        dfs[model] = {}

        # Loop through all attention mechanisms
        for att in d[model]:
            model_att = []
            for label in d[model][att]:
                label_arr = []
                for token_d in d[model][att][label]:
                    for feature in token_features:
                        label_arr.append(token_d[feature])
                model_att.append(label_arr)

            model_att_arr = np.array(model_att)
            df = pd.DataFrame(model_att_arr, index=row_index, columns=col_index)

            dfs[model][att] = df

    with open(os.path.join(path_results, 'tokens_phrases', f'token_energy_dict_mw{min_words}.pkl'), 'wb') as f:
        pickle.dump(dfs, f)


def get_k_tokens(models: List[str], atts: List[str], labels: List[str], min_words: int):
    if labels == ['all']:
        labels = mimic50_labels
    # Load k largest tokens per label for each model attention mechanism
    with open(os.path.join(path_results, 'tokens_phrases', f'token_energy_dict_mw{min_words}.pkl'), 'rb') as f:
        d_energy = pickle.load(f)

    tokens_all = []
    for model in models:
        for att in atts:
            tokens = d_energy[model][att].loc[labels, :].reset_index()
            tokens = tokens.rename(columns={'level_0': 'label',
                                            'level_1': 'desc',
                                            'level_2': 'freq',
                                            'level_3': 'quartile'})
            tokens.insert(0, 'model', [model] * len(labels))
            tokens.insert(1, 'att', [att] * len(labels))

            tokens_all.append(tokens)

    df = pd.concat(tokens_all).reset_index(drop=True)

    df.to_excel(os.path.join(path_results, 'tokens_phrases', f'Mimic50_largest_tokens_scores_mw{min_words}.xlsx'))


def check_label_in_Y(Y, label):
    if label in Y:
        return 1
    return 0


def load_prediction_files(models: List[str], atts: List[str], n_docs: int, seeds: List[str] = [13, 25, 42, 87, 111]):
    """

    Parameters
    ----------
    models : List[str]
        Model names
    atts : List[str]
        Attention mechanisms
    n_docs : int
        Number of documents in the dataset
    seeds : List[int]; default [13, 25, 42, 87, 111]
        List of seed numbers

    Returns
    -------
    Dict[str, Dict[str, Dict[str, np.array]]]

    """

    if os.path.isfile(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl')):
        with open(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl'), 'rb') as f:
            predictions_dict = pickle.load(f)
    else:
        params = {'CNN': '100_100_16_0.5_False_None',
                  'BiGRU': '256_100_16_0.5_None_None',
                  'BiLSTM': '512_200_16_0.5_None_None',
                  'CLF': '768_300_4_0.0_False_None'}

        predictions_dict = {}
        for model in models:
            predictions_dict[model] = {}
            for att in atts:
                predictions_dict[att] = {}
                preds_matrix = np.zeros((len(seeds), n_docs, 50), dtype=int)
                probs_matrix = np.zeros((len(seeds), n_docs, 50), dtype=float)
                for s, seed in enumerate(seeds):
                    # Retrieve predictions
                    file_name = f'{model}_Mimic50_{att}_{params[model]}_{seed}_test_preds.txt'
                    path_to_file = os.path.join(path_results, f'results_analysis_{model}', 'predictions', file_name)
                    with open(path_to_file, 'r') as f:
                        docs_preds = f.readlines()

                    # Retrieve probabilities
                    file_name = f'{model}_Mimic50_{att}_{params[model]}_{seed}_test_probs.txt'
                    path_to_file = os.path.join(path_results, f'results_analysis_{model}', 'predictions', file_name)
                    with open(path_to_file, 'r') as f:
                        docs_probs = f.readlines()

                    hadm_ids = []
                    for d, (doc_preds, doc_probs) in enumerate(zip(docs_preds, docs_probs)):

                        # deal with predictions
                        doc_preds = doc_preds.strip()
                        doc_preds_split = doc_preds.split('|')
                        hadm_id = doc_preds_split[0]
                        hadm_ids.append(hadm_id)
                        doc_preds = doc_preds_split[1:]

                        # deal with probabilities
                        doc_probs = doc_probs.strip()
                        doc_probs_split = doc_probs.split('|')
                        doc_probs = np.array(doc_probs_split[1:])

                        for l, label in enumerate(mimic50_labels):
                            probs_matrix[s][d] = doc_probs
                            for pred in doc_preds:
                                if label == pred:
                                    preds_matrix[s][d][l] = 1

                preds_matrix = np.round(np.mean(preds_matrix, axis=0))
                probs_matrix = np.median(probs_matrix, axis=0)

                predictions_dict[model][att] = {'preds': preds_matrix, 'probs': probs_matrix}
        with open(os.path.join(path_results, f'Mimic50_consolidated_preds_probs.pkl'), 'wb') as f:
            pickle.dump(predictions_dict, f)

    return predictions_dict


def retrieve_docs_with_labels(models, atts, labels: List[str]):
    """
    This function retrieves documents with ground-truth labels for all labels.

    Parameters
    ----------
    models : List[str]
        Model names
    atts : List[str]
        Attention mechanisms
    labels : List[str]
        List with labels

    Return
    ------
    List[str]
        List containing hadm_ids for each document associated with all labels

    """

    # Load base data set

    test_ids = pd.read_csv(os.path.join(path_mimic50, 'ids_Mimic50_test.csv')).HADM_ID.tolist()
    predictions_dict = load_prediction_files(models=models, atts=atts, n_docs=len(test_ids))

    df_50 = pd.read_pickle(os.path.join(path_mimic50, 'DATA_Mimic50.pkl'))
    test_ids = pd.read_csv(
        os.path.join(path_hadm50, f'hadm_ids_Mimic50_test.csv')).hadm_id.tolist()
    df_test = df_50[df_50['HADM_ID'].isin(test_ids)]

    df_Y = df_test[['HADM_ID', 'ICD9_CODE']]

    for label in labels:
        # retrieve index of label; index is connected with position of stored prediction and probability matrix
        idx = mimic50_labels.index(label)
        df_Y[label] = df_Y.apply(lambda x: check_label_in_Y(x.ICD9_CODE, label), axis=1)

    df_Y['sum'] = df_Y[labels].sum(axis=1)

    denominator = len(labels)
    while True:
        if df_Y[df_Y['sum'] == denominator].empty:
            denominator -= 1
        else:
            subset = df_Y[df_Y['sum'] == denominator]
            break

    doc_ids = subset.HADM_ID.tolist()

    return doc_ids


def get_energyscores_for_seq(dict_scores: Dict[str, Dict[str, float]], seq: List[int]):
    """
    Function that looks up energy scores for each token in the sequence and returns a sequence of token-specific
    energy scores.

    Parameters
    ----------
    dict_scores : Dict[str, Dict[str, float]]
        Dictionary that contains the energy score for each token for a specific attention mechanism or
        text encoder architecture
    seq : List[int]
        Integer-based sequences of the clinical documents

    Returns
    -------
    List[float]

    """

    return [dict_scores[idx]['mean'] for idx in seq]


def get_phrase_with_max_ave_scores(seq: List[float],
                                   phrase_sizes: List[int],
                                   get_att_scores: bool = False):
    """
    Function that returns the phrase with the largest averaged energy scores among phrases with 3(4), 4(6), or 5(8)
    tokens (integers reflect CLF tokens); The clinical longformer utilizes a byte-pair vocabulary and htus, has a larger
    token to word-ratio then the CNN and RNNs.

    Parameters
    ----------
    seq : List[float]
        List with energy scores, where each element is an energy score for a specific token
    phrase_sizes : List[int]
        List containing the sizes (number of tokens) of the phrases
    get_att_scores : bool; default=False
        FLag indicating if max phrase should be retrieved based on attention scores

    Returns
    -------
    Tuple(int, List[float])

    """

    # Loop through each phrase of the sequence
    if get_att_scores:
        seq = softmax(seq)
    max_phrases = []
    for ps in phrase_sizes:
        energy_means_per_phrase = []
        for idx in range(len(seq)):
            # compute mean and append to list
            phrase_mean = np.mean(seq[idx:idx + ps])
            energy_means_per_phrase.append(phrase_mean)

        # transform list to array to make use of numpy argmax function
        energy_means_per_phrase_arr = np.array(energy_means_per_phrase)

        # Get index of max phrase; index implicitly indicates the position of the first token
        starting_idx_of_max_phrase = np.argmax(energy_means_per_phrase_arr)

        # Retrieve phrase
        max_phrase = seq[starting_idx_of_max_phrase: starting_idx_of_max_phrase + ps]
        max_phrases.append((starting_idx_of_max_phrase, max_phrase))
    mean_energy_max_phrases = np.array([np.mean(phrase[1]) for phrase in max_phrases])
    max_phrase_all = max_phrases[np.argmax(mean_energy_max_phrases)]

    return max_phrase_all


def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0) # only difference


def get_phrase(model: str, seq: List[int], starting_idx_of_max_phrase: int, ps: int = 5):
    """
    Function that retrieves the tokens (words) for max phrase.

    Parameters
    ----------
    model : str
        Name of model
    seq : List[float]
        List with energy scores, where each element is an energy score for a specific token
    starting_idx_of_max_phrase : int
        Index of first token to indicate beginning of phrase
    ps : int; default=5
        Window size of phrase, e.g., how many tokens should be included

    Returns
    -------
    List[str]

    """

    # Get token indices from max phrase
    phrase_idx = seq[starting_idx_of_max_phrase: starting_idx_of_max_phrase + ps]

    # Look up tokens from vocabulary loaded in section 1.2
    if model == 'CLF':
        phrase_tokens = CLF_tokenizer.convert_ids_to_tokens(phrase_idx)
    else:
        phrase_tokens = mimic50_vocab.lookup_tokens(phrase_idx)
    return phrase_tokens


def get_context_of_phrase(model: str, seq: List[int], starting_idx_of_max_phrase: int, ps: int = 5, ws: int = 5):
    """
    Function that retrieves the tokens (words) for max phrase.

    Parameters
    ----------
    model : str
        Name of model
    seq : List[float]
        List with energy scores, where each element is an energy score for a specific token
    starting_idx_of_max_phrase : int
        Index of first token to indicate beginning of phrase
    ps : int; default=5
        Window size of phrase, e.g., how many tokens should be included

    Returns
    -------
    List[str]

    """

    # Get token indices from max phrase
    phrase_idx = seq[starting_idx_of_max_phrase - ws: starting_idx_of_max_phrase + ps + ws]

    if model == 'CLF':
        phrase_tokens = CLF_tokenizer.decode(phrase_idx)
    else:
        # Look up tokens from vocabulary loaded in section 1.2
        phrase_tokens = ' '.join(mimic50_vocab.lookup_tokens(phrase_idx))
    return phrase_tokens


def get_phrases_with_largest_score(models: List[str],
                                   atts: List[str],
                                   labels: str,
                                   split: str = 'test',
                                   doc_ids: List[int] = None,
                                   hier_cat_context_K: bool = False,
                                   hier_query_label_K: bool = False,
                                   get_att_scores: bool = False):
    """
    Function that retrieves the phrase of size ps with largest average energy score per document in dataset.

    Parameters
    ----------
    models : List[str]
        List of models of interest; e.g., ['CNN', 'BiGRU', 'BiLSTM', 'CLF']
    atts : List[str]
        List of attention mechanisms of interest;
        e.g., ['random', 'pretrained', 'hierarchical_random', 'hierarchical_pretrained']
    label : str
        Medical code of interest
    split : str; default='test'
        Dataset split, e.g., 'test', 'val'
    doc_ids : List[int]; default=None
        List containing dataframe indices for documents with ground truth labels associated with given labels to subset
        the dataset

    """

    if isinstance(doc_ids, list):
        pass
    else:
        doc_ids = [doc_ids]
    test_ids = pd.read_csv(os.path.join(path_mimic50, 'ids_Mimic50_test.csv')).HADM_ID.tolist()
    predictions_dict = load_prediction_files(models=models, atts=atts, n_docs=len(test_ids))  # df_test.shape[0])

    # Loop through all models and attention mechanisms that were specified
    for label in labels:
        data = []

        for model in models:
            path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

            # Sequences are different for the CNN,RNNs and the CLF
            if model == 'CLF':
                with open(os.path.join(path_mimic50, f'X_Mimic50_test_text.pkl'), 'rb') as f:
                    docs_tensor = pickle.load(f)['input_ids'].tolist()
                d = {'hadm_id': test_ids, 'token2id_max': docs_tensor}
                test_docs = pd.DataFrame(data=d)
                test_docs = test_docs[test_docs['hadm_id'].isin(doc_ids)]
                doc_length = 4096
                phrase_sizes = [4, 6, 8]

            else: # CNN and BiGRU / BiLSTM data

                test_docs = pd.DataFrame(
                    pd.read_pickle(os.path.join(path_mimic50, f'X_Mimic50_test.pkl')))
                test_docs['hadm_id'] = test_ids

                test_docs = test_docs[test_docs['hadm_id'].isin(doc_ids)]
                doc_length = 3000
                test_docs['token2id_max'] = test_docs.apply(lambda x: x.token2id[:doc_length], axis=1)
                phrase_sizes = [3, 4, 5]

            for att in atts:
                # Load model/att specific dictionary containing the energyscores for each token per label

                if hier_cat_context_K:
                    with open(os.path.join(path_res_energy, f'{model}_{att}_{split}_EC1K_energyscores.pkl'), 'rb') as f:
                        dict_scores = pickle.load(f)
                elif hier_query_label_K:
                    with open(os.path.join(path_res_energy, f'{model}_{att}_{split}_EQ2K_energyscores.pkl'), 'rb') as f:
                        dict_scores = pickle.load(f)
                else:
                    with open(os.path.join(path_res_energy, f'{model}_{att}_{split}_energyscores.pkl'), 'rb') as f:
                        dict_scores = pickle.load(f)

            # Loop through each specified label to extract the phrases with the largest energy scores
                for doc_idx in doc_ids:
                    seq = test_docs[test_docs['hadm_id']==doc_idx]['token2id_max'].iloc[0]

                    mean_energy_scores = get_energyscores_for_seq(dict_scores=dict_scores[label], seq=seq)

                    max_phrase_scores = get_phrase_with_max_ave_scores(seq=mean_energy_scores,
                                                                       phrase_sizes=phrase_sizes,
                                                                       get_att_scores=get_att_scores)
                    max_phrase_tokens = get_phrase(model=model,
                                                   seq=seq,
                                                   starting_idx_of_max_phrase=max_phrase_scores[0],
                                                   ps=len(max_phrase_scores[1]))

                    max_phrase_context = get_context_of_phrase(model=model,
                                                               seq=seq,
                                                               starting_idx_of_max_phrase=max_phrase_scores[0],
                                                               ps=len(max_phrase_scores[1]), ws=6)

                    data.append([model, att, label,
                                 mimic50_desc_dict[label],
                                 doc_idx, 1,
                                 predictions_dict[model][att]['preds'][test_ids.index(doc_idx)][mimic50_labels.index(label)],
                                 predictions_dict[model][att]['probs'][test_ids.index(doc_idx)][mimic50_labels.index(label)],
                                 max_phrase_scores, max_phrase_tokens, max_phrase_context])
        df = pd.DataFrame(data=data,
                          columns=['model', 'att_module', 'label', 'desc', 'doc_idx', 'y', 'y_hat', 'y_prob',
                                   'phrase_scores', 'phrase_tokens',
                                   'phrase_context'])
        df.to_excel(os.path.join(path_results, 'tokens_phrases', f'Mimic50_max_phrase_{label}.xlsx'))

    return df


def compute_energyscores_statistics(models: str,
                                    atts: str,
                                    min_words: int = 3,
                                    hier_cat_context_K: bool = False,
                                    hier_query_label_K: bool = False):

    stats_label = []
    stats_modelatt = []
    for model in models:
        path_res_energy = os.path.join(path_results, f'results_analysis_{model}', 'scores', 'energy_processed/')

        for att in atts:
            if hier_cat_context_K:
                with open(os.path.join(path_res_energy,
                                       f'{model}_{att}_test_EC1K_energyscores.pkl'), 'rb') as f:
                    d_scores = pickle.load(f)
            elif hier_query_label_K:
                with open(os.path.join(path_res_energy,
                                       f'{model}_{att}_test_EQ2K_energyscores.pkl'), 'rb') as f:
                    d_scores = pickle.load(f)
            else:
                with open(os.path.join(path_res_energy,
                                       f'{model}_{att}_test_energyscores.pkl'), 'rb') as f:
                    d_scores = pickle.load(f)
            vals_modelatt = []
            for label in d_scores.keys():
                vals = []
                for token in d_scores[label].keys():
                    if d_scores[label][token]['n'] >= min_words:
                        vals.append(d_scores[label][token]['mean'])
                        vals_modelatt.append(d_scores[label][token]['mean'])
                vals = np.array(vals, dtype=np.float64)
                summation = np.sum(np.array(list(map(abs, vals)), dtype=np.float64))
                mean = np.mean(vals)
                std = np.std(vals)
                min = np.min(vals)
                max = np.max(vals)
                CV = std / mean
                median = np.median(vals)
                Q25 = np.quantile(vals, 0.25)
                Q75 = np.quantile(vals, 0.75)
                n_pos_per = np.sum(vals >= 0) / len(vals)
                n_neg_per = np.sum(vals < 0) / len(vals)
                elements = [model, att, label,
                            np.round(summation, 3), np.round(mean, 3), np.round(std, 3),
                            np.round(CV, 3), np.round(min, 3), np.round(max, 3),
                            np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                            np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
                stats_label.append(elements)


            vals_modelatt = np.array(vals_modelatt, dtype=np.float64)
            summation = np.sum(np.array(list(map(abs, vals_modelatt)), dtype=np.float64))
            mean = np.mean(vals_modelatt)
            std = np.std(vals_modelatt)
            min = np.min(vals_modelatt)
            max = np.max(vals_modelatt)
            CV = abs(std) / abs(mean)
            median = np.median(vals_modelatt)
            Q25 = np.quantile(vals_modelatt, 0.25)
            Q75 = np.quantile(vals_modelatt, 0.75)
            n_pos_per = np.sum(vals_modelatt >= 0) / len(vals_modelatt)
            n_neg_per = np.sum(vals_modelatt < 0) / len(vals_modelatt)
            elements_modelatt = [model, att, np.round(summation, 3), np.round(mean, 3),
                                 np.round(std, 3), np.round(CV, 3),
                                  np.round(min, 3), np.round(max, 3),
                                 np.round(median, 3), np.round(Q25, 3), np.round(Q75, 3),
                                 np.round(n_pos_per, 3), np.round(n_neg_per, 3)]
            stats_modelatt.append(elements_modelatt)
            fig = plt.figure(figsize=(10, 8))
            plt.hist(vals_modelatt, bins=20)
            plt.savefig(os.path.join(path_results, 'energyscores', f'{model}_{att}_energyscores_distribution_mw{min_words}.png'))
            plt.close(fig)



    if hier_cat_context_K:
        df = pd.DataFrame(data=stats_label,
                          columns=['model', 'att_module', 'label', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_EC1K_descriptive_stats_label_mw{min_words}.xlsx'))
        df = pd.DataFrame(data=stats_modelatt,
                          columns=['model', 'att_module', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_EC1K_descriptive_stats_modelatt_mw{min_words}.xlsx'))
    elif hier_query_label_K:
        df = pd.DataFrame(data=stats_label,
                          columns=['model', 'att_module', 'label', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_EQ2K_descriptive_stats_label_mw{min_words}.xlsx'))
        df = pd.DataFrame(data=stats_modelatt,
                          columns=['model', 'att_module', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_EQ2K_descriptive_stats_modelatt_mw{min_words}.xlsx'))
    else:
        df = pd.DataFrame(data=stats_label,
                          columns=['model', 'att_module', 'label', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_label_mw{min_words}.xlsx'))

        df = pd.DataFrame(data=stats_modelatt,
                          columns=['model', 'att_module', 'sum', 'mean', 'std', 'CV', 'min', 'max',
                                   'median', 'Q25', 'Q75', 'n_pos_per', 'n_neg_per'])

        df.to_excel(os.path.join(path_results, 'energyscores', f'energyscore_descriptive_stats_modelatt_mw{min_words}.xlsx'))


import argparse

parser = argparse.ArgumentParser()
# Model-unspecific commandline arguments
parser.add_argument('-m', '--model',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['CNN', 'BiLSTM', 'BiGRU', 'CLF'],
                    help='Select a predefined model.')

parser.add_argument('-am', '--attention_module',
                    required=True,
                    type=str,
                    nargs='+',
                    choices=['random', 'hierarchical_random',
                             'pretrained', 'hierarchical_pretrained'],
                    help='Select a type of predefined attention mechanism or none.')

parser.add_argument('-pre', '--preprocessing',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_CK', '--pre_catcontext_K',
                    default=False,
                    type=bool,
                    choices=[True, False])
parser.add_argument('-pre_QK', '--pre_querylabel_K',
                    default=False,
                    type=bool,
                    choices=[True, False])

parser.add_argument('-kp', '--process_k_largest_tokens',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-k', '--k_token',
                    type=int,
                    default=3)
parser.add_argument('-mw', '--min_words',
                    type=int,
                    default=3)
parser.add_argument('-kg', '--get_k_largest_tokens',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-l', '--labels',
                    nargs='+',
                    default=['39.95', '36.15', '39.61', '427.31'])
parser.add_argument('-gp', '--get_phrases',
                    type=bool,
                    choices=[True, False])
parser.add_argument('-ps', '--phrase_size',
                    type=int,
                    default=5)
parser.add_argument('-di', '--doc_indices',
                    type=int,
                    nargs='+',
                    default=None)
parser.add_argument('-cs', '--compute_similarities',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-ges', '--get_energyscore_stats',
                    type=bool,
                    default=False,
                    choices=[True, False])
parser.add_argument('-gas', '--get_att_scores',
                    type=bool,
                    default=False,
                    choices=[True, False],
                    help='Flag indicating if most important phrases should be retrieved based on the attention scores')

args = parser.parse_args()


def main():
    models = args.model
    atts = args.attention_module
    pre = args.preprocessing
    pre_CK = args.pre_catcontext_K
    pre_QK = args.pre_querylabel_K
    kp = args.process_k_largest_tokens
    k = args.k_token
    mw = args.min_words
    k_get = args.get_k_largest_tokens
    labels = args.labels
    get_phrases = args.get_phrases
    ps = args.phrase_size
    doc_ids = args.doc_indices
    compute_similarities = args.compute_similarities
    ges = args.get_energyscore_stats
    get_att_scores = args.get_att_scores
    if pre:
        for model in models:
            for att in atts:
                t = time.process_time()
                load_scores(model=model, att=att, path_dataset=path_mimic50,
                            path_results=path_results, hier_cat_context_K=pre_CK, hier_query_label_K=pre_QK)
                consolidate_seed_dicts(model=model, att=att, path_results=path_results,
                                       hier_cat_context_K=pre_CK, hier_query_label_K=pre_QK)
                elapsed_time = time.process_time() - t
                print(f'Processing time: {elapsed_time}')

    if kp:
        d = {}
        for model in models:
            train_token_freqs = get_train_token_freqs(model=model, path_dataset=path_mimic50)
            d[model] = {}
            for att in atts:
                k_token_dict = get_k_largest_scores(model=model,
                                                    att=att,
                                                    train_token_freqs=train_token_freqs,
                                                    k=k,
                                                    min_words=mw)
                d[model][att] = k_token_dict
        create_dfs_from_dict(d, k=k, min_words=mw)

    if k_get:
        get_k_tokens(models=models, atts=atts, labels=labels, min_words=mw)

    if get_phrases:
        if doc_ids is None:
            doc_ids = retrieve_docs_with_labels(models=models, atts=atts, labels=labels)
            # doc_ids = np.random.choice(doc_ids, 1).tolist()
        print(f'Extract phrases for document with HADM-ID: {doc_ids}')
        get_phrases_with_largest_score(models=models,
                                       atts=atts,
                                       labels=labels,
                                       doc_ids=doc_ids,
                                       get_att_scores=get_att_scores)

    if ges:
        compute_energyscores_statistics(models=models,
                                        atts=atts,
                                        hier_cat_context_K=pre_CK,
                                        hier_query_label_K=pre_QK,
                                        min_words=mw)


if __name__ == '__main__':
    main()

