{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027f3639",
   "metadata": {},
   "source": [
    "# Notebook that enables the preprocessing of the MIMIC-III dataset\n",
    "- How to execute preprocessing of clinical notes: Cell -> Run all\n",
    "- The utility functions can be found in folder: /src/tools/utils_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8352663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/cmetzner/Desktop/Study/PhD/research/ORNL/Biostatistics and Multiscale System Modeling/attention_mechanisms\n"
     ]
    }
   ],
   "source": [
    "# Built-in libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# warnings are switched since they are not important for preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    root = os.path.dirname(os.path.dirname(oa.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root)\n",
    "print('Project root: {}'.format(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a1e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cmetzner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Installed libraries\n",
    "import pandas as pd\n",
    "\n",
    "# custom libraries\n",
    "from src.tools.utils_preprocess import rel2abs, preproc_clinical_notes\n",
    "from src.tools.utils_preprocess import create_splits, get_class_type\n",
    "#from src.tools.label_description import get_code_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c01f84",
   "metadata": {},
   "source": [
    "# Define some constants\n",
    "\n",
    "Subsets:\n",
    "- 'full': Full set of available codes in MIMIC-III\n",
    "- '50': 50 most frequent codes in MIMIC-III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656f32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_SIZE = 0.71\n",
    "TEST_SIZE = 0.52\n",
    "MIN_FREQ = 3\n",
    "\n",
    "subsets = ['MimicFull', 'Mimic50']\n",
    "splits = ['train', 'test', 'val']\n",
    "caml = True  # Use splits and preprocessing published by Mullenbach et al. 2018 (https://github.com/jamesmullenbach/caml-mimic)\n",
    "\n",
    "# Change paths to location of raw data (PROCEDURES_ICD.csv and DIAGNOSES_ICD.csv)\n",
    "path_data_raw = os.path.join(root, 'data', 'raw')\n",
    "# Change paths to location of processed data\n",
    "path_data_proc = os.path.join(root, 'data', 'processed')\n",
    "# Change paths to location where code descriptions are stored (CMS32_DESC_LONG_SHORT_DX.xlsx and CMS32_DESC_LONG_SHORT_SG.xlsx)\n",
    "path_data_external = os.path.join(root, 'data', 'external')\n",
    "\n",
    "\n",
    "# Create directories to store dataset specific data\n",
    "for subset in subsets:\n",
    "    if not os.path.exists(os.path.join(path_data_proc, f'data_{subset}/')):\n",
    "        os.makedirs(os.path.join(path_data_proc, f'data_{subset}/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819776a",
   "metadata": {},
   "source": [
    "# Preprocess ICD-9 Procedure/Diagnoses Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df3b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load ICD-9 procedure and diagnosis codes\n",
    "df_proc = pd.read_csv(os.path.join(path_data_raw, 'PROCEDURES_ICD.csv'))\n",
    "df_diag = pd.read_csv(os.path.join(path_data_raw, 'DIAGNOSES_ICD.csv'))\n",
    "\n",
    "# Remove all rows that have no ICD9-codes\n",
    "df_proc = df_proc[~df_proc['ICD9_CODE'].isna()]\n",
    "df_diag = df_diag[~df_diag['ICD9_CODE'].isna()]\n",
    "\n",
    "print(f'Shape of procedure data: {df_proc.shape}')\n",
    "print(f\"Number of unique patient ids (SUBJECT_ID): {df_proc.SUBJECT_ID.nunique()}\")\n",
    "print(f\"Number of unique hospital admission ids (HADM_ID): {df_proc.HADM_ID.nunique()}\")\n",
    "print(f\"Number of unique ICD-9 procedure codes: {df_proc.ICD9_CODE.nunique()}\")\n",
    "print(f'Shape of diagnosis data: {df_diag.shape}')\n",
    "print(f\"Number of unique patient ids (SUBJECT_ID): {df_diag.SUBJECT_ID.nunique()}\")\n",
    "print(f\"Number of unique hospital admission ids (HADM_ID): {df_diag.HADM_ID.nunique()}\")\n",
    "print(f\"Number of unique ICD-9 diagnosis codes: {df_diag.ICD9_CODE.nunique()}\")\n",
    "\n",
    "print(f'\\nTotal number of ICD-9 codes in raw data of MIMIC-III: {df_proc.ICD9_CODE.nunique() + df_diag.ICD9_CODE.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7335b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure ICD-9 codes are strings\n",
    "df_proc['ICD9_CODE'] = df_proc['ICD9_CODE'].astype(str)\n",
    "df_diag['ICD9_CODE'] = df_diag['ICD9_CODE'].astype(str)\n",
    "\n",
    "# Remove all whitespace\n",
    "df_proc['ICD9_CODE'] = df_proc.apply(lambda x: x.ICD9_CODE.strip(), axis=1)\n",
    "df_diag['ICD9_CODE'] = df_diag.apply(lambda x: x.ICD9_CODE.strip(), axis=1)\n",
    "\n",
    "# Transform relative ICD-9 code representation to absolute code\n",
    "df_proc['ABS_CODE'] = df_proc.apply(lambda x: rel2abs(x.ICD9_CODE, flag_proc=True), axis=1)\n",
    "df_diag['ABS_CODE'] = df_diag.apply(lambda x: rel2abs(x.ICD9_CODE, flag_proc=False), axis=1)\n",
    "\n",
    "df_proc['ICD9_TYPE'] = 'procedure'\n",
    "df_diag['ICD9_TYPE'] = 'diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ef22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dataframes containing procedure and diagnosis codes\n",
    "df_codes = pd.concat([df_diag, df_proc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee716e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save codes to csv file\n",
    "df_codes.to_csv(os.path.join(path_data_proc, 'ALL_CODES.csv'), index=False,\n",
    "                columns=['SUBJECT_ID', 'HADM_ID', 'ABS_CODE', 'ICD9_TYPE'],\n",
    "                header=['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ICD9_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fd4ff",
   "metadata": {},
   "source": [
    "## Check number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv(os.path.join(path_data_proc, 'ALL_CODES.csv'), dtype={'ICD9_CODE': str})\n",
    "codes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Billable ICD-9 codes\n",
    "codes.ICD9_CODE.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591e653",
   "metadata": {},
   "source": [
    "# Preprocess and tokenize clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clinical notes dataset\n",
    "notes = pd.read_csv(os.path.join(path_data_raw, 'NOTEEVENTS.csv'))\n",
    "notes = notes.loc[notes.CATEGORY == 'Discharge summary']\n",
    "notes = notes[['SUBJECT_ID', 'HADM_ID', 'TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = preproc_clinical_notes(df_notes=notes,\n",
    "                               path_data_proc=path_data_proc,\n",
    "                               caml_clean=caml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e9c8c",
   "metadata": {},
   "source": [
    "## Take a Look at the cleaned and tokenized clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea412b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_pickle(os.path.join(path_data_proc, 'CLEANED_NOTES.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e750c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c5cb3",
   "metadata": {},
   "source": [
    "# Combine Labels with Clinical Notes\n",
    "- Investigate all procedure codes that have just 2 positions and see if we can change them to a more meaningful version\n",
    "\n",
    "\n",
    "To-Do's\n",
    "- Split datasets into full, 50\n",
    "- Split SUBJECT_ID and HADM_ID into train, test, val - split is based on SUBJECT_ID --> Avoid data leakage\n",
    "- Retrieve categories (everything before the .)\n",
    "- Create X (map token2idx)\n",
    "- Retrieve list of labels BASED ON given DATASET\n",
    "- Create y (map codes and cats to multi-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hospital admissions with clinical notes\n",
    "print(f'Number of hospital admission ids with clinical notes: {notes.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Align HADM_IDS of codes and notes\n",
    "# Some HADM_ID have no codes but no notes\n",
    "# Some HADM_ID have notes but no codes (we already filtered out the HADM_ID without codes)\n",
    "\n",
    "# Get all hospital admission ids that have notes\n",
    "hadm_notes = notes.HADM_ID.unique().tolist()\n",
    "\n",
    "# Retrieve codes for those hospital admission ids\n",
    "codes = codes[codes['HADM_ID'].isin(hadm_notes)]\n",
    "\n",
    "# Check number of hospital admission id's in codes dataset\n",
    "print(f'Number of hospital admission ids that have codes and notes: {codes.HADM_ID.nunique()}')\n",
    "\n",
    "# Apparently, we have 4 hospital admission id's that have notes but no codes\n",
    "# Let's filter them out.\n",
    "# Get hospital admission ids with notes but are not included in the codes dataframe\n",
    "display(notes[~notes['HADM_ID'].isin(codes.HADM_ID.unique().tolist())])\n",
    "\n",
    "hadm_notes_wo_codes = notes[~notes['HADM_ID'].isin(codes.HADM_ID.unique().tolist())].HADM_ID.tolist()\n",
    "print(hadm_notes_wo_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[~notes['HADM_ID'].isin(hadm_notes_wo_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7748ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check if both dataframes have same number of hospital admission id's\n",
    "print('Check if number of HADM_IDs are equal in both dataframes.')\n",
    "print(f'Codes == Notes : {codes.HADM_ID.nunique()} == {notes.HADM_ID.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e091c",
   "metadata": {},
   "source": [
    "# Create datasets for experiments by subsetting HADM_ID\n",
    "- full: \n",
    "- 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d711854",
   "metadata": {},
   "source": [
    "## Dataset: 'full' set of codes in MIMIC-III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc00b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_full = codes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_codes_full = sorted(codes_full.ICD9_CODE.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of billable code labels: {len(l_codes_full)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data_proc, f'data_MimicFull', 'l_codes_MimicFull.pkl'), 'wb') as f:\n",
    "    pickle.dump(l_codes_full, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate individual label mentions in lists\n",
    "codes_full = codes_full.groupby(['SUBJECT_ID', 'HADM_ID']).agg({'ICD9_CODE': list}).reset_index()\n",
    "\n",
    "# Remove redundant labels in lists\n",
    "codes_full['ICD9_CODE'] = codes_full.apply(lambda x: sorted(list(set(x.ICD9_CODE))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfcdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full dataset\n",
    "df_full = codes_full.merge(notes, how='left', on=['SUBJECT_ID', 'HADM_ID']).sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "# Save dataset\n",
    "df_full.to_pickle(os.path.join(path_data_proc, 'data_MimicFull', 'DATA_MimicFull.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4649cc7",
   "metadata": {},
   "source": [
    "## Dataset: n='50' most frequent in MIMIC-III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "# Retrieve the 50 most frequent codes in MIMIC-III\n",
    "most_50 = sorted(codes.ICD9_CODE.value_counts()[:n].index.tolist())\n",
    "most_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset codes dataset\n",
    "codes_50 = codes.copy()\n",
    "codes_50 = codes_50[codes_50['ICD9_CODE'].isin(most_50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list with labels\n",
    "l_codes_50 = sorted(codes_50.ICD9_CODE.unique().tolist())\n",
    "print(f'Number of billable code labels: {len(l_codes_50)}')\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(path_data_proc, 'data_Mimic50', 'l_codes_Mimic50.pkl'), 'wb') as f:\n",
    "    pickle.dump(l_codes_50, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c8414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate individual label mentions in lists\n",
    "codes_50 = codes_50.groupby(['SUBJECT_ID', 'HADM_ID']).agg({'ICD9_CODE': list}).reset_index()\n",
    "\n",
    "# Remove redundant labels in lists\n",
    "codes_50['ICD9_CODE'] = codes_50.apply(lambda x: sorted(list(set(x.ICD9_CODE))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb83668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full dataset\n",
    "df_50 = codes_50.merge(notes, how='left', on=['SUBJECT_ID', 'HADM_ID']).sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "# Save dataset\n",
    "df_50.to_pickle(os.path.join(path_data_proc, 'data_Mimic50', 'DATA_Mimic50.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2f9eb",
   "metadata": {},
   "source": [
    "# Generate training/testing/validation splits (X and y samples)\n",
    "In this segment we are splitting the four individual datasets into training/testing/validation sets using the patient ids (SUBJECT_ID) and HADM_ID. By using SUBJECT_ID we are avoiding data leackage from one split to another, i.e., one distinct patient only occurs in either one of those splits.\n",
    "\n",
    "For sake of reproducibility, for the `full` and `50` datasets we are using the published HADM_ID's proposed by Mullenbach et al. 2018, since a plethora of published work used them as the foundation (source: https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7cfa4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current subset: MimicFull\n",
      "Current split: train\n",
      "Vocab size for train: 120481\n",
      "Current split: test\n",
      "Current split: val\n",
      "\n",
      "Current subset: Mimic50\n",
      "Current split: train\n",
      "Vocab size for train: 50176\n",
      "Current split: test\n",
      "Current split: val\n"
     ]
    }
   ],
   "source": [
    "# This cell preprocesses the tokenized data\n",
    "for subset in subsets:\n",
    "    create_splits(subset=subset, path_data_proc=path_data_proc, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81903e9f",
   "metadata": {},
   "source": [
    "# Get Code Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2402cd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for subset in subsets:            \n",
    "#    get_code_desc(path_data_external=path_data_external, path_data_processed=path_data_proc, subset=subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe07a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7269f34d",
   "metadata": {},
   "source": [
    "# Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c3e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e192532",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root, 'data', 'processed', 'data_Mimic50', 'X_Mimic50_test_text.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca21706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10296,  2236,  ...,   135, 17713,   102],\n",
       "        [  101, 10296,  2236,  ...,  1918,   117,   102],\n",
       "        [  101, 10296,  2236,  ...,  5674, 18901,   102],\n",
       "        ...,\n",
       "        [  101, 10296,  2236,  ...,   131,  3258,   102],\n",
       "        [  101, 10296,  2236,  ...,   119,   176,   102],\n",
       "        [  101, 10296,  2236,  ...,   170,  1248,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8680da07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] admission date : deidentified discharge date : deidentified date of birth : deidentified sex : f service : medicine allergies : levofloxacin attending : deidentified chief complaint : dyspnea, pneumonia major surgical or invasive procedure : deidentified et tube change deidentified arterial line placement deidentified right ij line placement deidentified ir - guided picc placement deidentified trach placement deidentified peg placement deidentified picc removed for fungemia deidentified single lumen picc placed history of present illness : year - old woman with history of asthma, copd, dm, hypothyroidism, with recent history significant for worsening dyspnea over past three months, status post four courses of antibiotics and steroids for presumed copd exacerbation, presenting to deidentified hospital on deidentified with acute worsening dyspnea, intubated for respiratory distress, and transferred to deidentified for further management. as noted above, she has a history of worsening dyspnea over the past few months that has been treated with antibiotics and steroids. she presented to osh on deidentified with worsening dyspnea and had a fever to 103f, and was started on ceftriaxone / azithromycin ( deidentified ) for pneumonia and copd exacerbation. she developed an increasing oxygen requirement, however, and initially required nasal canula ( % on l o2 ) but later required face mask ( % on l on deidentified ). her cxr, which initially was read as negative for infiltrate, progressively worsening, with increasingly prominent diffuse bilateral infiltrates, right greater than left. she also had a negative chest cta. with an oxygen requirement that was rising and worsening dyspnea, she was transitioned to vanc / zosyn / levo and then vanc / zosyn / ceftaz ( deidentified ; ceftaz started deidentified ; levo on deidentified ) because of an allergy to levofloxacin. it is unclear why she received double coverage for gram negatives. she was also given methylprednisolone ( deidentified dose mg iv q8 ; then 80 - > mg [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6192eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea27ed2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(val[idx] \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems())}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "sample = {f'{key}': torch.tensor(val[idx] for key, val in data.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bba8888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 10296,  2236,   131, 21357, 11951,  6202, 12398,  2236,   131,\n",
       "        21357, 11951,  6202,  2236,  1104,  3485,   131, 21357, 11951,  6202,\n",
       "         2673,   131,   175,  1555,   131,  3621, 26423, 16426,  6617,  1665,\n",
       "         1155,  1200, 19310,   131, 28117,  9654,  1161,   113, 28117,  9654,\n",
       "         7637,  3080,  4704,   114,   120,   170,  3457,  1665, 14185,  3161,\n",
       "         1105, 18952,   120, 17496,  7231,  6546,   131, 21357, 11951,  6202,\n",
       "         2705, 12522,   131,  2229,   169,   169,  3600,  1757,   112,   112,\n",
       "         1558, 13467,  1137, 19849,  7791,   131,  3839,  1607,  1104,  1675,\n",
       "         6946,   131, 26063,  2130,   188,   120,   185,  3532,  1107,   172,\n",
       "         1775,  1150,  2756,  1114,  2229,  3600,  1757,  1120, 21357, 11951,\n",
       "         6202,   117,  1400,  5855,  8961,  1105,  1108,  1276,  1106,  1138,\n",
       "         1107,   188,  5208,  1204, 24438, 16071, 18071,  1548,  1113, 21357,\n",
       "        11951,  6202,   119,   185,  1204,  1108,  2786,  1113, 15242,   176,\n",
       "         3069,  1115,  1108,  1195,  6354,  1181,  1228,   119, 16278,  1107,\n",
       "         1103,  1168,  2704,  2799,  8828,   182,  1197,  1105,  5199,  1112,\n",
       "          119,   185,  1204,  1110,  3175,  1106, 21357, 11951,  6202,  1111,\n",
       "         1748,  2635,   119,  1763,  2657,  1607,   131,  1763,  2657,  1607,\n",
       "          181,  4455,  1361, 14044, 25669, 15391, 10024, 14410,   181,  4455,\n",
       "         1361,   176,  2858,  4027, 23643,  8043,  8167, 10721,   177,  1204,\n",
       "         1179,   177, 24312,  8401,  2897, 25710, 14183,  1465,  1884, 15789,\n",
       "         1616, 18593,  3653,   188,   120,   185,   188,  5208,  1204, 21357,\n",
       "        11951,  6202,  1286,   172,  3161, 19172, 23445,  1775,   176, 25081,\n",
       "         1607,   188,  8840,  2897,  1107,  1103,  1763, 23448,  2941, 15818,\n",
       "         6620,  1114,  1231, 25461,  1104,  1103,  1884,  4934,  1114,  1224,\n",
       "        10442,  1268,  2071,  1103,  5656,  1821, 19675,   170,  1214,  3718,\n",
       "          182, 24129,  8974,  1934,  1607,   131, 26369, 21357, 11951,  6202,\n",
       "          118,  2491,  1114,  5787,  7490,  1534,  1266,  1607,   131,  1664,\n",
       "         7235, 19091, 16442,  4649,  2952, 12211,   131,  2952, 12211,   170,\n",
       "         8124, 27647,  1513,  8561,   131,   117,  1231, 20080,   131,   184,\n",
       "         1477,  2068,   131,   110,   171,   120,   185,  1268,   131,  6745,\n",
       "          120,  2539,  1286,   131,  3976,   131,  2841,   131,  1704,   131,\n",
       "         2241,   131,  3712,   164,   193,   166,  9964,   164,   193,   166,\n",
       "         1119,  3452,   131,  1679, 17670,  1161,   164,   193,   166,   174,\n",
       "        18882,   164,   193,   166,  2455,   131, 28117,  8661,  1513,   164,\n",
       "          193,   166,  1554,   187,  4165,   164,   193,   166,  2229,   131,\n",
       "         8682,  2330, 20557,  1193,   164,   193,   166,  1762,   131,   187,\n",
       "        11096,   164,   193,   166, 12692,   164,   166, 22895, 14701,   131,\n",
       "         2991,   164,   193,   166,  1664,   118,  4267, 15874,  4902,   164,\n",
       "          193,   166,  1664,   118,  8886,   164,   193,   166,  7125,  1883,\n",
       "         3807,   116,   164,   193,   166,  4252,  7877,  9084,  1905,   131,\n",
       "         3258,   164,   193,   166,   117,  1218,   118,  1679, 21089,   164,\n",
       "          193,   166,  1325,  1162,   117,   187,  1513,   171,  1968,  1218,\n",
       "        15884,  5048, 14494, 15661, 10658, 28060,   131,  3839,   164,   193,\n",
       "          166, 24928, 11955,   131, 10272,  1193,  9964, 27631,   131,   175,\n",
       "         5521, 17536,  1268,   131,   116,   124,  1286,   131,   116,   124,\n",
       "          173,  1643,  1268,   131,   116,   123,  1286,   131,   116,   123,\n",
       "          185,  1204, 21357, 11951,  6202,   131,   116,   123,  1286,   131,\n",
       "          116,   123, 17172,  1268,   131,   116,   123,  1286,   131,   116,\n",
       "          123,  1679, 24123,  2686,   131, 21357, 11951,  6202,  1275,   131,\n",
       "         2532,  2312,  1892,   192,  1830,  1665,   118,  1275,   119,   127,\n",
       "          187,  1830,  1665,   118,   124,   119,  3565,   115,   177,  1403,\n",
       "         1830,   102])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed948e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11813a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
